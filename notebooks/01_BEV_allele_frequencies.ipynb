{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making allele frequency plots\n",
    "\n",
    "This notebook contains code to process the CRISPResso \"allele frequency table\" files from base editor validation experiments. The inputs are 2 input files: the first contains metainformation about each sample to make the \"allele frequency\" file, and the second contains metainformation to compute correlations between the log-normalized read counts. The output of this file are 3 files for each sgRNA / primer pair: \n",
    "1. a file containing all alleles and their read counts for each sample\n",
    "2. a filtered version of (1) that only contains alleles with at least 1% abundance in any sample\n",
    "3. a file containing the Pearson correlations between log-normalized read counts of each allele with > 100 reads in at least one sample\n",
    "\n",
    "(1) is the starting file used to show the abundance of specific edits over time (code in BEV_aa_over_time.ipynb). (2) is the starting file used to create allele-level heatmaps (this was done using GraphPad Prism). (3) is the starting file for the plots showing the replicate correlation for validation experiments (actual plots were made using GraphPad Prism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_14324357633586325432() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_14324357633586325432()\">Show/Hide Toggle Function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(toggle_text_addon = '', for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "    \n",
    "    toggle_text = 'Show/Hide' + ' ' + toggle_text_addon  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "hide_toggle(toggle_text_addon='Toggle Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import core_functions as cfs\n",
    "from math import log\n",
    "import os\n",
    "from os import path\n",
    "import itertools\n",
    "import gpplot as gpp\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "gpp.set_aesthetics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "print('Python version: ' + sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.2.1\n",
      "numpy 1.19.5\n"
     ]
    }
   ],
   "source": [
    "modules = ['pandas', 'numpy']\n",
    "for module in modules:\n",
    "    try:\n",
    "        print(module + ' ' + sys.modules[module].__version__)\n",
    "    except:\n",
    "        print(module + ' has no __version__ attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_3583873136402941293() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_3583873136402941293()\">Show/Hide clean_input_file function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This function removes any NaN rows from input_file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "df: metainformation file data frame\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "df: metainformation file data frame with any empty/NaN rows removed\n",
    "\n",
    "'''\n",
    "def clean_input_file(df):\n",
    "    df = df.dropna() #drop NaN\n",
    "    if 'BEV_start' in df.columns:\n",
    "        df = df.drop('BEV_start', axis=1)\n",
    "    if 'BEV_end' in df.columns:\n",
    "        df = df.drop('BEV_end', axis=1)\n",
    "    #dropna() converts int to float, so convert them back\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns #select subset of df of type float\n",
    "    for col in float_cols: \n",
    "        #get original column index so can replace at correct loc\n",
    "        index = df.columns.get_loc(col)\n",
    "        #rename float cols as \"float_\"col name \n",
    "        float_col_name = 'float_' + col\n",
    "        df = df.rename(columns = {col : float_col_name})\n",
    "        #overwrite as type int\n",
    "        float_to_int = df[float_col_name].astype(int).copy()\n",
    "        df.insert(index, col, float_to_int)\n",
    "        #drop float column \n",
    "        df = df.drop(float_col_name, axis = 1)\n",
    "    return df\n",
    "\n",
    "hide_toggle(toggle_text_addon='clean_input_file function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_10578699359925531503() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_10578699359925531503()\">Show/Hide remove_utr function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes UTRs from Aligned_Sequence before translation\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "translation_ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without UTRs and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "def remove_utr(translation_ref_seq, aligned, rev_com):\n",
    "    if ('[' not in translation_ref_seq) and (']' not in translation_ref_seq): # if no UTR\n",
    "        return\n",
    "    intron_flag = False # default value for intron flag is False\n",
    "    if any(char for char in translation_ref_seq if char.islower()): # check for any introns\n",
    "        intron_flag = True\n",
    "        translation_ref_seq = translation_ref_seq.upper()\n",
    "    \n",
    "    # check if number of open brackets equals number of closed brackets\n",
    "    if translation_ref_seq.count('[') != translation_ref_seq.count(']'):\n",
    "        print('Unequal number of open and closed brackets. Please make sure all brackets are closed.')\n",
    "        return\n",
    "    open_bracket = False # True when encounters open bracket [\n",
    "    closing_bracket = False # True when encounters closed bracket ]\n",
    "    in_utr = False # True when open_bracket == True and closing_bracket == False\n",
    "    utr_list = []\n",
    "    utr = ''\n",
    "    utr_start_end_list = []\n",
    "    start_end = [] \n",
    "    bracket_count = 0\n",
    "    # check to make sure all open brackets are only followed by closed, and closed is never followed by closed\n",
    "    for idx, char in enumerate(translation_ref_seq):\n",
    "        if char == '[':\n",
    "            if open_bracket: # if already encountered open bracket\n",
    "                print('Open bracket encountered after open bracket. Please make sure all brackets are closed.')\n",
    "                return\n",
    "            else:\n",
    "                open_bracket = True\n",
    "                closing_bracket = False\n",
    "                \n",
    "        if char == ']':\n",
    "            if open_bracket: # if this ] closes encountered open bracket\n",
    "                open_bracket = False\n",
    "                closing_bracket = True\n",
    "                bracket_count +=1 # count closed bracket \n",
    "                in_utr = False # end of UTR\n",
    "                utr_list.append(utr) # add UTR sequence to list\n",
    "\n",
    "                start_end.append(start_end[0]+len(utr)- 1)\n",
    "\n",
    "                utr_start_end_list.append(start_end)\n",
    "                utr = '' # Reset UTR\n",
    "                start_end = [] # Rest start_end index pair\n",
    "                \n",
    "            elif closing_bracket: # if already encountered closing bracket\n",
    "                print('Closing bracket encountered after closing bracket. Please check input.')\n",
    "                return\n",
    "            else:\n",
    "                print('No open bracket encountered. Please check input.')\n",
    "                return\n",
    "        \n",
    "        if (open_bracket and not closing_bracket) and (char != '[') and (char != ']'):\n",
    "            in_utr = True \n",
    "            # if first character in utr, store start index\n",
    "            if len(utr) == 0:\n",
    "                bracket_count +=1 # count open bracket\n",
    "                start_end.append(idx-bracket_count) # subtract # brackets encountered\n",
    "            utr = utr + char\n",
    "    \n",
    "    new_aligned = aligned \n",
    "    new_aligned_list = []\n",
    "    new_utrs = []\n",
    "    new_utr_start_end_list = []\n",
    "    # get length of translation_ref_seq without brackets\n",
    "    len_translation_ref_seq = len(translation_ref_seq.replace('[','').replace(']',''))\n",
    "    \n",
    "    if rev_com: # need to reverse complement UTR from translation_ref_seq to match aligned seq\n",
    "        # Reverse UTR index start, end positions if rev_com = True\n",
    "        for num, i in enumerate(utr_start_end_list):\n",
    "            start = i[0]\n",
    "            end = i[1]\n",
    "\n",
    "            new_idx_start = -(end+1) # reverse index, +1 b/c starts at -1 in reverse\n",
    "        \n",
    "            # convert to positive index by adding length of full translation_ref_seq (without brackets)\n",
    "            new_idx_start = len_translation_ref_seq + new_idx_start\n",
    "\n",
    "            new_idx_end = new_idx_start + len(utr_list[num]) - 1\n",
    "\n",
    "            new_utr_start_end_list.append([new_idx_start, new_idx_end])\n",
    "#       \n",
    "        for utr in utr_list:\n",
    "            rev_com_utr = cfs.revcom(utr)\n",
    "            new_utrs.append(rev_com_utr)\n",
    "    else:\n",
    "        new_utr_start_end_list = utr_start_end_list\n",
    "        new_utrs = utr_list\n",
    "    \n",
    "    if intron_flag: # if sequence contains both UTR(s) and intron(s)\n",
    "        utr_df = pd.DataFrame()\n",
    "\n",
    "        for i, start_end in enumerate(new_utr_start_end_list):\n",
    "            utr_df.loc[i, 'start'] = int(start_end[0])\n",
    "            utr_df.loc[i, 'end'] = int(start_end[1])\n",
    "            utr_df.loc[i, 'sequence'] = new_utrs[i].upper()\n",
    "            utr_df.loc[i, 'type'] = 'UTR'\n",
    "            \n",
    "        return utr_df\n",
    "    \n",
    "    else:\n",
    "        len_prev_utr = 0\n",
    "        for idx, utr in enumerate(new_utrs):\n",
    "            # adjust start and end index based on length of prev UTR, because indices shift after removal of UTR\n",
    "            # if first UTR, len_prev_utr = 0\n",
    "            start_end_idx = new_utr_start_end_list[idx]\n",
    "\n",
    "            if not rev_com: # if rev_com, UTRs removed in from end of sequence then upstream so earlier indices not affected\n",
    "                start_end_idx = [start_end_idx[0]-len_prev_utr, start_end_idx[1]-len_prev_utr]\n",
    "\n",
    "            # Check if bases at utr positions in aligned seq match utr\n",
    "            new_aligned_utr = new_aligned[start_end_idx[0]:start_end_idx[1]+1]\n",
    "\n",
    "            if new_aligned_utr != utr:\n",
    "\n",
    "                new_aligned_list.append('UTR')\n",
    "\n",
    "            # remove UTR based on index position \n",
    "            if start_end_idx[0] == 0: # if UTR at the beginning of new_aligned seq, keep substring after end_index\n",
    "                new_aligned = new_aligned[start_end_idx[1]+1:]\n",
    "            else: # (bases until start index + bases after end index)\n",
    "                new_aligned = new_aligned[:start_end_idx[0]]+new_aligned[start_end_idx[1]+1:]\n",
    "\n",
    "            len_prev_utr = len_prev_utr+len(utr) \n",
    "\n",
    "        if rev_com: # Now that UTR has been found, reverse complement again so correct translation input\n",
    "            new_aligned = cfs.revcom(new_aligned)\n",
    "\n",
    "        new_aligned_list.append(new_aligned)\n",
    "        return new_aligned_list\n",
    "    \n",
    "hide_toggle(toggle_text_addon='remove_utr function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_11470144994566412435() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_11470144994566412435()\">Show/Hide remove_introns function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes introns from Aligned_Sequence before translation\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "translation_ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without introns and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def remove_introns(translation_ref_seq, aligned, rev_com):\n",
    "\n",
    "    utr_flag = False # default value for utr_flag is False\n",
    "    if ('[' in translation_ref_seq) or (']' in translation_ref_seq):\n",
    "        utr_flag = True\n",
    "        translation_ref_seq = translation_ref_seq.replace('[','').replace(']','')\n",
    "        \n",
    "    # get index positions of introns \n",
    "    intron_idx= [i for i, base in enumerate(translation_ref_seq) if base.islower()]\n",
    "\n",
    "    intron_idx_list = []\n",
    "    # Get consecutive indices to group indices for same intron\n",
    "    for k, g in groupby(enumerate(intron_idx), lambda ix : ix[0] - ix[1]):\n",
    "         intron_idx_list.append(list(map(itemgetter(1), g)))\n",
    "    \n",
    "    # if no introns, exit function\n",
    "    if not intron_idx_list:\n",
    "\n",
    "        return \n",
    "    \n",
    "    # Get sequence of intron to check for intronic mutations \n",
    "    introns = []\n",
    "    intron_start_end_list = []\n",
    "    for intron_idx_group in intron_idx_list:\n",
    "        intron_start = intron_idx_group[0]\n",
    "        intron_end = intron_idx_group[-1]\n",
    "        # store intron start and end index positions in pairs in list \n",
    "        intron_start_end_list.append([intron_idx_group[0], intron_idx_group[-1]])\n",
    "\n",
    "        intron = translation_ref_seq[intron_start:intron_end+1]\n",
    "        introns.append(intron)\n",
    "\n",
    "    new_aligned = aligned.upper()\n",
    "    new_aligned_list = []\n",
    "    \n",
    "    new_intron_start_end_list = []\n",
    "    new_introns = []    \n",
    "\n",
    "    if rev_com:\n",
    "        # Reverse intron index positions if rev_com = True\n",
    "        for i in intron_start_end_list:\n",
    "            new_idx_start = -(i[1]+1) # reverse index\n",
    "            # convert to positive index\n",
    "            new_idx_start = len(translation_ref_seq) + new_idx_start\n",
    "            new_idx_end = -(i[0]+1) # reverse index\n",
    "            # convert to positive index\n",
    "            new_idx_end = len(translation_ref_seq) + new_idx_end\n",
    "            new_intron_start_end_list.append([new_idx_start, new_idx_end])\n",
    "\n",
    "        for i in introns:\n",
    "            rev_com_intron = cfs.revcom(i.upper())\n",
    "            new_introns.append(rev_com_intron)\n",
    "\n",
    "    else:\n",
    "        new_intron_start_end_list = intron_start_end_list\n",
    "        new_introns = introns\n",
    "    \n",
    "    if utr_flag: # if sequence contains both UTR(s) and introns(s), pass data to combo function\n",
    "        intron_df = pd.DataFrame()\n",
    "\n",
    "        for i, start_end in enumerate(new_intron_start_end_list):\n",
    "            intron_df.loc[i, 'start'] = int(start_end[0])\n",
    "            intron_df.loc[i, 'end'] = int(start_end[1])\n",
    "            intron_df.loc[i, 'sequence'] = new_introns[i].upper()\n",
    "            intron_df.loc[i, 'type'] = 'intron'\n",
    "        return intron_df\n",
    "\n",
    "    else:    \n",
    "\n",
    "        len_prev_intron = 0\n",
    "        for idx, intron in enumerate(new_introns):\n",
    "            # if not first intron, adjust start and end index based on length of prev intron\n",
    "            start_end_idx = new_intron_start_end_list[idx]\n",
    "            if not rev_com:\n",
    "                start_end_idx = [start_end_idx[0]-len_prev_intron, start_end_idx[1]-len_prev_intron]\n",
    "\n",
    "            # Check if bases at intron positions in aligned seq match intron\n",
    "            if new_aligned[start_end_idx[0]:start_end_idx[1]+1] != intron.upper():\n",
    "                new_aligned_list.append('intron')\n",
    "\n",
    "            else:\n",
    "                # remove intron based on index position \n",
    "                if start_end_idx[0] == 0: # if intron at the beginning of new_aligned seq, keep substring after end_index\n",
    "                    new_aligned = new_aligned[start_end_idx[1]+1:]\n",
    "\n",
    "                else: # (bases until intron start index + bases after intron end index)\n",
    "                    new_aligned = new_aligned[:start_end_idx[0]]+new_aligned[start_end_idx[1]+1:]\n",
    "\n",
    "                len_prev_intron = len_prev_intron + len(intron)\n",
    "\n",
    "        if rev_com:\n",
    "            new_aligned = cfs.revcom(new_aligned)\n",
    "\n",
    "        new_aligned_list.append(new_aligned)\n",
    "        return new_aligned_list\n",
    "\n",
    "hide_toggle(toggle_text_addon='remove_introns function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_3016922335511388251() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_3016922335511388251()\">Show/Hide remove_utr_intron_combo Full Function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes introns and UTRs from Aligned_Sequence before translation if translation_ref_seq contains both\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "translation_ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without introns and UTRS and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "\n",
    "def remove_utr_intron_combo(translation_ref_seq, aligned, rev_com):\n",
    "#     print(translation_ref_seq.upper())\n",
    "    utr_df = remove_utr(translation_ref_seq, aligned, rev_com)\n",
    "    intron_df = remove_introns(translation_ref_seq, aligned, rev_com)\n",
    "    \n",
    "#     print(utr_df, intron_df)\n",
    "    # Store UTR and intron start/end positions and sequences in single df\n",
    "    removal_df = pd.concat([utr_df, intron_df]).sort_values(by='start').reset_index(drop=True)\n",
    "    \n",
    "    removal_df['start'] = removal_df['start'].copy().astype(int)\n",
    "    removal_df['end'] = removal_df['end'].copy().astype(int)\n",
    "    prev_start = 0\n",
    "    prev_end = 0\n",
    "    prev_type = ''\n",
    "    \n",
    "    new_aligned = aligned\n",
    "    new_aligned_list = []\n",
    "    len_prev_removal = 0\n",
    "    \n",
    "    for idx, removal_seq in enumerate(removal_df['sequence']):\n",
    "        removal_row = removal_df.loc[idx, :]\n",
    "        # if not first removal region (UTR/intron), adjust start and end index based on length of prev removal\n",
    "        start_end_idx = [removal_row['start'], removal_row['end']]\n",
    "\n",
    "        # Since start positions are ordered in ascending order, adjust based on length of prev removal \n",
    "        # regardless of rev_com\n",
    "\n",
    "        start_end_idx = [start_end_idx[0]-len_prev_removal, start_end_idx[1]-len_prev_removal]\n",
    "\n",
    "        # Check if bases at removal positions in aligned seq match removal_seq\n",
    "        if new_aligned[start_end_idx[0]:start_end_idx[1]+1] != removal_seq:\n",
    "            if removal_row['type'] == 'UTR':\n",
    "                new_aligned_list.append('UTR')\n",
    "            if removal_row['type'] == 'intron':\n",
    "                new_aligned_list.append('intron')\n",
    "\n",
    "        else:\n",
    "            # remove region based on index position \n",
    "            if start_end_idx[0] == 0: # if region at the beginning of new_aligned seq, keep substring after end_index\n",
    "                new_aligned = new_aligned[start_end_idx[1]+1:]\n",
    "            else: # (bases until intron start index + bases after intron end index)\n",
    "                new_aligned = new_aligned[:start_end_idx[0]]+new_aligned[start_end_idx[1]+1:]\n",
    "            len_prev_removal = len_prev_removal + len(removal_seq)\n",
    "\n",
    "    if rev_com:\n",
    "        new_aligned = cfs.revcom(new_aligned)\n",
    "    new_aligned_list.append(new_aligned)\n",
    "    return new_aligned_list\n",
    "\n",
    "\n",
    "hide_toggle(toggle_text_addon='remove_utr_intron_combo Full Function')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_3947457911461343497() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_3947457911461343497()\">Show/Hide translate function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: translates aligned sequence after removal of introns and/or UTRs\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "seq_list: list containing aligned sequence after removal of introns and/or UTRs \n",
    "as well as any edited intron and/or UTR regions (if applicable)\n",
    "frame: frame for translation as described in metainformation file documentation\n",
    "first_codon: first codon to be translated as described in metainformation file documentation\n",
    "last_codon: last codon to be translated as described in metainformation file documentation\n",
    "codon_map: dictionary mapping codon sequences to single letter amino acid annotation\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "translation_list: list containing translated aligned sequence as well as \"Possible intronic mutation\" \n",
    "and/or \"Possible UTR mutation\" where applicable\n",
    "\n",
    "'''\n",
    "\n",
    "# This function returns the translation of a given sequence and frame\n",
    "def translate(seq_list, frame, first_codon, last_codon, codon_map):\n",
    "    translation_list = []\n",
    "    for seq in seq_list:\n",
    "        if seq == 'UTR':\n",
    "            translation_list.append('Possible UTR mutation')\n",
    "            continue\n",
    "        if seq == 'intron':\n",
    "            translation_list.append('Possible intronic mutation')\n",
    "            continue\n",
    "        \n",
    "        aa = ''\n",
    "        i = 0\n",
    "        # if frame is not 1, add nucleotides necessary to beginning of sequence for translation based on first_codon\n",
    "        if frame == 2:\n",
    "            seq = first_codon[0] + seq\n",
    "            frame = 1\n",
    "        if frame == 3:\n",
    "            seq = first_codon[0:2] + seq\n",
    "            frame = 1\n",
    "\n",
    "        while i < len(seq):\n",
    "            substring = ''\n",
    "            while frame <= 3:\n",
    "                if i<len(seq):\n",
    "                    if seq[i] == '-': # deletion\n",
    "                        i += 1\n",
    "                        # frame doesn't change\n",
    "                    else:\n",
    "                        substring += seq[i]  \n",
    "                        i += 1\n",
    "                        frame+=1\n",
    "                else: # if reached end of the sequence and frame still <=3, complete codon sequence based on last_codon\n",
    "                    substring += last_codon[frame-1]\n",
    "                    i += 1\n",
    "                    frame+=1\n",
    "\n",
    "            if len(substring) == 3:\n",
    "                frame = 1 # reset frame \n",
    "                if ('N' in substring):\n",
    "                    aa = aa + '-'\n",
    "                else:\n",
    "                    aa = aa + codon_map[substring] # translate codon\n",
    "    #             print(aa)\n",
    "            else:\n",
    "                frame = 1\n",
    "    #         print(frame)\n",
    "    \n",
    "        translation_list.append(aa)\n",
    "#         return aa\n",
    "    return translation_list\n",
    "\n",
    "codon_map = {'TTT':'F', 'TTC':'F', 'TTA':'L', 'TTG':'L', 'CTT':'L', 'CTC':'L', 'CTA':'L', 'CTG':'L', 'ATT':'I', 'ATC':'I',\n",
    "             'ATA':'I', 'ATG':'M', 'GTT':'V', 'GTC':'V', 'GTA':'V', 'GTG':'V', 'TCT':'S', 'TCC':'S', 'TCA':'S', 'TCG':'S',\n",
    "             'CCT':'P', 'CCC':'P', 'CCA':'P', 'CCG':'P', 'ACT':'T', 'ACC':'T', 'ACA':'T', 'ACG':'T', 'GCT':'A', 'GCC':'A',\n",
    "             'GCA':'A', 'GCG':'A', 'TAT':'Y', 'TAC':'Y', 'TAA':'*', 'TAG':'*', 'CAT':'H', 'CAC':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "             'AAT':'N', 'AAC':'N', 'AAA':'K', 'AAG':'K', 'GAT':'D', 'GAC':'D', 'GAA':'E', 'GAG':'E', 'TGT':'C', 'TGC':'C',\n",
    "             'TGA':'*', 'TGG':'W', 'CGT':'R', 'CGC':'R', 'CGA':'R', 'CGG':'R', 'AGT':'S', 'AGC':'S', 'AGA':'R', 'AGG':'R',\n",
    "             'GGT':'G', 'GGC':'G', 'GGA':'G', 'GGG':'G'}\n",
    "\n",
    "hide_toggle(toggle_text_addon='translate function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_12465155514370201280() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_12465155514370201280()\">Show/Hide input check functions</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Function: checks formatting and content of inputs for metainformation file and prints error messages if applicable\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "input_file: metainformation file\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "\"Input file is correct!\" or \"Please address errors listed above before proceeding.\"\n",
    "\n",
    "'''\n",
    "\n",
    "def check_input_file(input_file):\n",
    "    \n",
    "    print('Checking input file...')\n",
    "    # Check each input column for correct formatting\n",
    "\n",
    "    err_flag = False\n",
    "\n",
    "    for i, row in tqdm(input_file.iterrows()):\n",
    "        translation_err_flag = False\n",
    "        utr_flag = False\n",
    "        intron_flag = False\n",
    "        sg = row['sg']\n",
    "        # sgRNA sequence must be all uppercase\n",
    "        sgRNA_seq = row['sgRNA_sequence'].upper()\n",
    "        # translation_ref_seq: use WT aligned condition to check formatting, frame, first codon, last codon\n",
    "        ref_seq = row['translation_ref_seq']\n",
    "        frame = row['frame']\n",
    "        # First and last codon must also be all uppercase (should be all coding)\n",
    "        first_codon = row['first_codon'].upper()\n",
    "        last_codon = row['last_codon'].upper()\n",
    "        \n",
    "        # Make sure there are only As, Cs, Gs, and Ts in the sequence inputs \n",
    "        sequence_inputs = [sgRNA_seq, ref_seq, first_codon, last_codon]\n",
    "        for seq in sequence_inputs:\n",
    "            chars_allowed = ['A', 'C', 'G', 'T', '[', ']']\n",
    "            non_nuc_char = [char for char in seq if char.upper() not in chars_allowed]\n",
    "            if non_nuc_char: # if there are any characters that are not A, C, G, or T or brackets\n",
    "                print(non_nuc_char)\n",
    "                print('Invalid character in sequence. Please make sure the sequence only contains As, Cs, Gs, and Ts.')\n",
    "                return\n",
    "            else:\n",
    "                # Brackets only allowed in translation_ref_seq\n",
    "                if ((('[' in seq) or (']' in seq)) and (seq != ref_seq)):\n",
    "                    print('Invalid character in sequence. Brackets are only allowed in translation_ref_seq.')\n",
    "                    return\n",
    "                \n",
    "        \n",
    "        print('sg:', sg)\n",
    "        \n",
    "        input_file.loc[i, 'sgRNA_sequence'] = sgRNA_seq\n",
    "        \n",
    "        print('Translation reference sequence:', ref_seq)\n",
    "        WT_aligned_seq = ref_seq\n",
    "        brackets = '[]'\n",
    "        # if UTR in translation_ref_seq\n",
    "        if ('[' in ref_seq) or (']' in ref_seq):\n",
    "            utr_flag = True\n",
    "            for bracket in brackets:\n",
    "                WT_aligned_seq = WT_aligned_seq.replace(bracket, '')\n",
    "        if any(c.islower() for c in ref_seq):\n",
    "            intron_flag = True\n",
    "            WT_aligned_seq = WT_aligned_seq.upper()\n",
    "        \n",
    "        \n",
    "        translation_input = WT_aligned_seq\n",
    "\n",
    "        # Remove UTRs and/or introns if applicable to check frame, first codon, and last codon \n",
    "        # Since translation_ref_seq in correct orientation for translation, rev_com = False for this check\n",
    "        rev_com = False\n",
    "\n",
    "        if utr_flag and intron_flag:\n",
    "            translation_input_list = remove_utr_intron_combo(ref_seq, translation_input, rev_com)\n",
    "        elif utr_flag:\n",
    "            translation_input_list = remove_utr(ref_seq, translation_input, rev_com)\n",
    "        elif intron_flag:\n",
    "            translation_input_list = remove_introns(ref_seq, translation_input, rev_com)\n",
    "        \n",
    "        # Match frame and translation_input with first codon to check if they match\n",
    "        translation_start_idx = frame - 1\n",
    "        print('Translation input:', translation_input_list)\n",
    "        translation_input = translation_input_list[0]\n",
    "        # Check if first base in translation_input matches corresponding nucleotide in first codon based on frame \n",
    "        if translation_input[0] != first_codon[translation_start_idx]:\n",
    "            print(translation_input[0], first_codon[translation_start_idx])\n",
    "            print('Frame does not match base in first_codon. Please check inputs') \n",
    "            err_flag = True\n",
    "#             translation_err_flag = \n",
    "        else:\n",
    "            # Check if last codon in translation_ref_seq is complete\n",
    "            # length of translation_input - # bases in start codon present in translation_ref_seq\n",
    "            print(len(translation_input), translation_start_idx)\n",
    "            len_last_codon = (len(translation_input) - (3-translation_start_idx))%3 \n",
    "\n",
    "            if len_last_codon == 0:\n",
    "                if translation_input[-3:] != last_codon:\n",
    "                    print(translation_input[-3:], last_codon)\n",
    "                    print('Input for last_codon does not align with translation_ref_seq. Please check inputs.')\n",
    "                    err_flag = True\n",
    "            else:\n",
    "                if translation_input[-len_last_codon:] != last_codon[:len_last_codon]:\n",
    "                    print(translation_input[-len_last_codon:], last_codon[:len_last_codon])\n",
    "                    print('Input for last_codon does not align with frame and translation_ref_seq. Please check inputs.')\n",
    "                    err_flag = True\n",
    "\n",
    "            if not err_flag:\n",
    "                print('Expected WT Translation: ', translate(translation_input_list, frame, first_codon, last_codon, codon_map))\n",
    "\n",
    "    if not err_flag:\n",
    "        return('Input file is correct!')\n",
    "    else:\n",
    "        return('Please address errors listed above before proceeding.')\n",
    "    \n",
    "# Check to make sure folder filepaths are correctly formatted\n",
    "def check_folder_filepath(filepath):\n",
    "    # Check if filepath is absolute\n",
    "    if (path.isabs(filepath)):\n",
    "        # if absolute, make sure it starts with a '/'\n",
    "        if filepath[0] != '/':\n",
    "            filepath = '/' + filepath\n",
    "    else:# if relative, get rid of '/' at the beginning of filepath\n",
    "        if filepath[0] == '/':\n",
    "            filepath = filepath[1:]\n",
    "    if filepath[-1] != '/':\n",
    "        filepath = filepath+'/'\n",
    "    # Check if filepath exists\n",
    "    if path.exists(filepath):\n",
    "        return filepath\n",
    "    else: # if filepath does not exist, create folders in filepath\n",
    "        if 'CRISPResso' not in filepath:\n",
    "            os.makedirs(filepath)\n",
    "        else:\n",
    "            raise ValueError('Filepath does not exist. Please check your inputs.')\n",
    "        return filepath\n",
    "\n",
    "hide_toggle(toggle_text_addon='input check functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_9794399558601054955() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_9794399558601054955()\">Show/Hide process data functions</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: converts the BEV number from an int to a 3-digit string\n",
    "'''\n",
    "def get_bev_str(bev):\n",
    "    bev = int(bev)\n",
    "    if bev < 10:\n",
    "        return '00'+str(bev)\n",
    "    if bev < 100:\n",
    "        return '0'+str(bev)\n",
    "    return str(bev)\n",
    "'''\n",
    "Called from: get_path\n",
    "Function: Verifies existence of filepath generated by get_path function to retrieve 'Alleles_frequency_table_around_sgRNA' file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "def check_filepath(filepath,bev,primer,sg_seq):\n",
    "    file_loc = filepath+'CRISPResso_on_'+bev+'_'+primer+'/'+'Alleles_frequency_table_around_sgRNA_'+sg_seq+'.txt'\n",
    "    print(file_loc)\n",
    "    if path.exists(file_loc):\n",
    "        return file_loc\n",
    "    else:\n",
    "        raise ValueError('File not found. Please check your filepath inputs.')\n",
    "        return ''\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: calls check_filepath to get filepath where CRISPResso output files are stored, \n",
    "whose folder path (CRISPResso_filepath) is provided above\n",
    "Calls: check_filepath\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_num : BEV sample number\n",
    "primer: primer name (from input file)\n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "\n",
    "def get_path(bev_num,primer,sg_seq):\n",
    "    bev = bev_string_id + '_' + get_bev_str(bev_num)\n",
    "    filepath = CRISPResso_filepath\n",
    "    return check_filepath(filepath,bev,primer,sg_seq)\n",
    "\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: merges together the \"Allele_frequency_table_around_sgRNA\" files\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "existing_df: merge data frame with 'Aligned_Sequence' and 'Reference_Sequence' columns defined in get_bev_files\n",
    "cols: empty list populated in function \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with Aligned_Sequence, Reference_Sequence, Reads columns from each sample \n",
    "cols: columns labeled with BEV sample number \n",
    "'''\n",
    "def merge_bev_file(filepath,bev,sg_seq,existing_df,cols):\n",
    "    if not path.exists(filepath):\n",
    "        print('no file')\n",
    "        return existing_df,cols\n",
    "    df = pd.read_table(filepath,index_col=False)\n",
    "    # Sum together any rows that share both 'Aligned Sequence' and 'Reference Sequence' with each other (this is rare)\n",
    "    df_summed = df[['Aligned_Sequence','Reference_Sequence','#Reads','%Reads']].groupby(['Aligned_Sequence','Reference_Sequence'],as_index=False).agg('sum')\n",
    "    cols.append(str('_BEV_'+str(bev)))\n",
    "    df_summed = df_summed.rename(columns={'#Reads':str('#Reads_BEV_'+str(bev)),'%Reads':str('%Reads_BEV_'+str(bev))})\n",
    "    # Outer merge onto existing dataframe\n",
    "    merge = pd.merge(existing_df,df_summed,how='outer',on=['Aligned_Sequence','Reference_Sequence'])\n",
    "    # Fill in nans with 0\n",
    "    merge = merge.fillna(0)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: function gets and merges all \"Allele_frequency_table_around_sgRNA\" files for a given sgRNA sequence\n",
    "(i.e. different replicates, drug conditions, etc.)\n",
    "This is customized to work with files with the \"BEV\" notation\n",
    "Calls: get_path, merge_bev_file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_list : contains BEV sample number, primer name, guide sequence for each sample  \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge: merged dataframe with \n",
    "cols:\n",
    "\n",
    "'''\n",
    "\n",
    "def get_bev_files(bev_list):\n",
    "    merge = pd.DataFrame(columns=['Aligned_Sequence','Reference_Sequence'])\n",
    "    cols = []\n",
    "    for bev,sg_seq,primer_name in bev_list:\n",
    "        filepath = get_path(bev,primer_name,sg_seq)\n",
    "        if filepath != '':\n",
    "            merge,cols = merge_bev_file(filepath=filepath,bev=bev,sg_seq=sg_seq,existing_df=merge,cols=cols)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: filters out rows that don't meet given threshold (<1% for %Reads and <100 for #Reads)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row in column to which function is being applied (%Reads or #Reads)\n",
    "cols: given cols (%Reads, #Reads for all samples )\n",
    "val: threshold for filter \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns False for any rows (alleles) that have a value < the given value in ALL of the given cols\n",
    "\n",
    "'''\n",
    "def read_count_filter(row,cols,val):\n",
    "    for col in cols:\n",
    "        if row[col] > val:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "'''\n",
    "This function checks for WT allele (if Aligned_Sequence = Reference_Sequence)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of merge dataframe \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns True if the allele is unedited (i.e. WT) and False otherwise\n",
    "\n",
    "'''\n",
    "def get_wt_col(row):\n",
    "    if row['Aligned_Sequence'] == row['Reference_Sequence']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''\n",
    "This function calculates the LFC\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of metainformation input file containing BEV_test and BEV_ref columns\n",
    "data_file : merged dataframe containing log-normalized rpm for each allele\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "data_file : merged dataframe, now with LFC columns for each BEV test / ref pair\n",
    "\n",
    "'''\n",
    "def get_lfc_v2(row,data_file):\n",
    "    cols = []\n",
    "    bev_list = row['BEV_test'].split(';')\n",
    "    \n",
    "    # Go through each test sample in BEV_test column\n",
    "    for i,bev in enumerate(bev_list):\n",
    "        test = get_bev_str(bev)\n",
    "        \n",
    "        # Get reference sample for LFC from BEV_ref column\n",
    "        ref = get_bev_str(row['BEV_ref'].split(';')[i])\n",
    "        \n",
    "        # Calculate LFC\n",
    "        data_file['LFC_'+test+'-'+ref] = data_file['#Reads_BEV_'+test+';lognorm'] - data_file['#Reads_BEV_'+ref+';lognorm']  \n",
    "        cols.append('LFC_'+test+'-'+ref)\n",
    "        \n",
    "    # Average together LFC columns\n",
    "    data_file['AvgLFC_'+'_'.join(bev_list)] = data_file.loc[:,cols].mean(axis=1)\n",
    "    return data_file\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: merges read counts (filtered), lognorms, aligned sequences, reference sequences, translations from \n",
    "test and reference samples for sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "data : deduplicated input file with column 'sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with filtered read counts, lognorms, aligned sequences, reference sequences, translations\n",
    "\n",
    "'''\n",
    "        \n",
    "def process_data_v2(data):\n",
    "    bev_list = [] # list to store info for retrieving CRISPResso files for given sgRNA\n",
    "    ref_nums = [] # list to store BEV numbers for CRISPResso files for reference samples \n",
    "    \n",
    "    for i,row in data.iterrows():\n",
    "        # combine BEV_ref and BEV_test sample numbers and loop through BEV numbers to get CRISPResso files\n",
    "        BEV_ref = data['BEV_ref'].copy().tolist() \n",
    "        BEV_ref_split = BEV_ref[0].split(';')\n",
    "        BEV_test = data['BEV_test'].copy().tolist() \n",
    "        BEV_test_split = BEV_test[0].split(';')\n",
    "        BEV_nums = BEV_ref_split + BEV_test_split\n",
    "        \n",
    "        for bev_num in BEV_nums:\n",
    "            # store BEV number, sgRNA sequence, and primer name to get corresponding CRISPResso files \n",
    "            bev_list.append((get_bev_str(bev_num),row['sgRNA_sequence'],row['primer']))\n",
    "    \n",
    "    # Call get_bev_files function which merges Alleles_frequency_tables_around_sgRNA for given sgRNA  \n",
    "    merge,cols = get_bev_files(bev_list)\n",
    "    \n",
    "    # Calculate log-normalized reads per million for each col\n",
    "    for col in ['#Reads'+col for col in cols]:\n",
    "        colsum = merge[col].sum()\n",
    "        merge.loc[:,str(col+';lognorm')] = merge[col].apply(lambda x: log((float(x)/float(colsum))*1000000 + 1,2))\n",
    "\n",
    "    print(['%Reads'+col for col in cols])\n",
    "    #changed to <2% for poor sequencing quality\n",
    "    # Apply read count filter\n",
    "    merge.loc[:,'%read_count_filter'] = merge.apply(read_count_filter,args=(['%Reads'+col for col in cols],2),axis=1) # less than 2% of all reads\n",
    "    merge.loc[:,'#read_count_filter'] = merge.apply(read_count_filter,args=(['#Reads'+col for col in cols],100),axis=1) # less than 100 reads\n",
    "    \n",
    "    # Before translating, check if there are introns and/or UTRs in translation_ref_seq column in input file\n",
    "    # introns indicated by lowercase letters in translation_ref_seq column \n",
    "    # UTRs indicated by square brackets in translation_ref_seq column \n",
    "    \n",
    "    # Returns true if intron in translation_ref_seq (i.e. if lowercase letters in input translation_ref_seq)\n",
    "    intron_flag = any(c.islower() for c in str(data.loc[data.index[0], 'translation_ref_seq']))\n",
    "    \n",
    "    # Returns true if UTR in translation_ref_seq (i.e. if square brackets in input translation_ref_seq)\n",
    "    utr_flag = any(c=='[' for c in str(data.loc[data.index[0], 'translation_ref_seq']))\n",
    "    \n",
    "    if utr_flag or intron_flag:\n",
    "        filtered_input_df = pd.DataFrame()\n",
    "        filtered_input_df['old_Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].copy()\n",
    "        filtered_input_df['translation_ref_seq'] = data['translation_ref_seq'].to_list()[0]\n",
    "        filtered_input_df['rev_com'] = data.loc[i, 'rev_com']\n",
    "        \n",
    "    # If translated_ref_seq contains both UTRs and introns, call remove_utr_intron_combo function\n",
    "    if utr_flag and intron_flag:\n",
    "        print('both introns and UTRs exist')\n",
    "        filtered_input_df['Aligned_Sequence']= list(map(remove_utr_intron_combo, filtered_input_df['translation_ref_seq'], filtered_input_df['Aligned_Sequence'], filtered_input_df['rev_com']))\n",
    "    \n",
    "    # If translated_ref_seq contains only UTRs, call remove_utr function\n",
    "    elif utr_flag:\n",
    "        print('only UTRs exist')\n",
    "        \n",
    "        # Call remove_utrs function to remove UTRs from aligned sequence before translating \n",
    "        filtered_input_df['Aligned_Sequence']= list(map(remove_utr, filtered_input_df['translation_ref_seq'], filtered_input_df['old_Aligned_Sequence'].copy(), filtered_input_df['rev_com']))\n",
    "\n",
    "    # If translated_ref_seq contains only introns, call remove_introns function\n",
    "    elif intron_flag:\n",
    "        print('only introns exist')\n",
    "        # Call remove_introns function to remove introns from aligned sequence before translating \n",
    "        filtered_input_df['Aligned_Sequence']= list(map(remove_introns, filtered_input_df['translation_ref_seq'], filtered_input_df['old_Aligned_Sequence'], filtered_input_df['rev_com']))\n",
    "\n",
    "    # if introns or UTR in translation_ref_seq\n",
    "    if utr_flag or intron_flag: \n",
    "        # Call translate function to translate new (UTR-free and intron-free) sequence  \n",
    "        merge.loc[:,'Translated'] = filtered_input_df['Aligned_Sequence'].apply(translate, args =(row['frame'],row['first_codon'], row['last_codon'], codon_map,))\n",
    "    \n",
    "    # if no introns or UTR\n",
    "    else:\n",
    "        print('no UTRs or introns')\n",
    "        # check if Aligned Sequence needs to be reverse complemented before translating \n",
    "        rev_com = data.loc[i, 'rev_com']\n",
    "        if rev_com:\n",
    "            merge.loc[:,'Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].apply(cfs.revcom)\n",
    "            merge.loc[:,'Reference_Sequence'] = merge.loc[:,'Reference_Sequence'].apply(cfs.revcom)\n",
    "        # Call translate function to translate Aligned Sequence (reverse complemented if necessary) \n",
    "        merge.loc[:,'Translated'] = merge.loc[:,'Aligned_Sequence'].apply(translate,args=(row['frame'],row['first_codon'], row['last_codon'], codon_map,))\n",
    "    \n",
    "    return merge\n",
    "'''\n",
    "Called from: run\n",
    "Function: joins all possible combinations of #Reads;lognorms columns from samples for given sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : #Reads;lognorm columns \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "columns joined by '_' : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column'\n",
    "Ex. '#Reads_BEV_041;lognorm_#Reads_BEV_042;lognorm'  \n",
    "\n",
    "'''\n",
    "\n",
    "def get_corr_name(row):\n",
    "    cols = [row['R1'],row['R2']]\n",
    "    cols.sort()\n",
    "    return '_'.join(cols)\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: gets combinations of samples specified in reps_for_correlation column in correlation input \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "corr_input : correlation input file \n",
    "sg: guide identifier from corr_input\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "combos : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column' for pairs specified in corr_input  \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def get_correlation_cols(corr_input,sg):\n",
    "    corr_input = corr_input.loc[corr_input['sg'] == sg,:]\n",
    "    combos = []\n",
    "    for i,r in corr_input.iterrows():\n",
    "        bevs = r['reps_for_correlation'].split(';')\n",
    "        bevs = ['#Reads_BEV_'+get_bev_str(bev)+';lognorm' for bev in bevs]\n",
    "        if len(bevs) > 1:\n",
    "            combos.extend(itertools.combinations(bevs,2))\n",
    "    combos = ['_'.join(combo) for combo in combos]\n",
    "    return combos\n",
    "\n",
    "hide_toggle(toggle_text_addon='process data functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_7262240699120808849() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_7262240699120808849()\">Show/Hide run function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This function calls all previous functions to generate output tables \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "input_file : input file with columns 'sg', 'translation_ref_seq', 'sgRNA_sequence', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "corr_input : input file with columns 'sg', 'reps_for_correlation' described above\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "output files stored in output_filepath given above\n",
    "\n",
    "'''\n",
    "def run(input_file,corr_input):\n",
    "    \n",
    "    # List of sgRNA identifiers provided in \"sg\" column of input file\n",
    "    sg_list = list(set(input_file['sg'].tolist())) # drop duplicates from list\n",
    "\n",
    "    # Go through each sgRNA separately    \n",
    "    for sg in sg_list:\n",
    "        print(sg)\n",
    "        \n",
    "        # Filter input file to contain only rows for given sgRNA\n",
    "        data = input_file.loc[input_file['sg'] == sg,:]\n",
    "        \n",
    "        # Merge all the allele read counts ('BEV_ref' and 'BEV+_test' samples)\n",
    "        # Then drop duplicate rows\n",
    "        data_dedup = data.drop_duplicates(subset=['sg', 'sgRNA_sequence','primer','frame','rev_com', 'BEV_ref', 'BEV_test'])\n",
    "#         data_dedup = data.drop_duplicates(subset=['sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'])\n",
    "\n",
    "        \n",
    "        # Call process_data_v2 function which adds the following columns to output table:\n",
    "        #read counts, lognorms, aligned sequences, reference sequences, translations \n",
    "        merge = process_data_v2(data_dedup)\n",
    "        \n",
    "        # Get the WT column\n",
    "        merge['WT'] = merge.apply(get_wt_col,axis=1)\n",
    "        \n",
    "#         print(merge[merge['WT']])\n",
    "        # Now, go through each row and calculate the LFC for each of the pairs specified in BEV_test and BEV_ref\n",
    "        for i,r in data.iterrows():               \n",
    "            merge = get_lfc_v2(r,merge)\n",
    "        print('num rows in merge df:', len(merge))\n",
    "        print(merge[merge['%read_count_filter'] == True])\n",
    "            \n",
    "        # Write out 2 files: full file (merge) and filtered file (only including alleles with > 2% reads in at least one condition)\n",
    "        Path(output_filepath).mkdir(parents=True, exist_ok=True)\n",
    "        merge.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_allele_frequency_table_around_sgRNA.csv',index=False)\n",
    "        filtered = merge[merge['%read_count_filter'] == True]\n",
    "        filtered.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_filtered_allele_frequency_table_around_sgRNA.csv',\n",
    "                        index=False)            \n",
    "        \n",
    "        filtered_adjusted_percentage = filtered.copy()\n",
    "        \n",
    "        # Adjust percentages in filtered table so they total 100% \n",
    "        \n",
    "        # Get #Reads columns from filtered table \n",
    "        num_reads_cols_all = [col for col in filtered_adjusted_percentage.columns if '#Reads' in col]\n",
    "        # List lognorm column names that are included in #Reads cols \n",
    "        lognorm_cols = [col for col in num_reads_cols_all if 'lognorm' in col]\n",
    "        # Drop lognorm columns \n",
    "        num_reads_cols = [col for col in num_reads_cols_all if col not in lognorm_cols]\n",
    "        percentage_reads_cols = []\n",
    "#         print(num_reads_cols)\n",
    "        \n",
    "        for col in num_reads_cols:\n",
    "            num_reads_col = col\n",
    "            sum_reads = filtered_adjusted_percentage[num_reads_col].sum()\n",
    "            print(sum_reads)\n",
    "#             percentage_reads_col ='%Reads_BEV_'+ get_bev_str(num)\n",
    "            percentage_reads_col = num_reads_col.replace('#', '%')\n",
    "            percentage_reads_cols.append(percentage_reads_col)\n",
    "            for i in filtered_adjusted_percentage.index:\n",
    "                num_reads = filtered_adjusted_percentage.loc[i, num_reads_col]\n",
    "                adjusted_percentage_reads = (num_reads/sum_reads) * 100 \n",
    "                filtered_adjusted_percentage.at[i, percentage_reads_col] = adjusted_percentage_reads\n",
    "        \n",
    "        # Drop rows with %Reads < 1% in all %Reads columns after adjustment \n",
    "        filtered_adjusted_percentage_2 = filtered_adjusted_percentage.copy()\n",
    "        \n",
    "        for i, row in filtered_adjusted_percentage_2.iterrows():\n",
    "            percent_condition = row[percentage_reads_cols]<1\n",
    "            print('percent condition', percent_condition)\n",
    "            if percent_condition.all():\n",
    "                # if all percent reads columns for allele are < 1%, drop row\n",
    "                filtered_adjusted_percentage_2 = filtered_adjusted_percentage_2.drop(i, axis=0)\n",
    "        \n",
    "        print(filtered_adjusted_percentage_2)\n",
    "                    \n",
    "        # Make sure WT is included\n",
    "        if filtered_adjusted_percentage_2[filtered_adjusted_percentage_2['WT'].eq(True)].empty:\n",
    "            WT_row = filtered_adjusted_percentage[filtered_adjusted_percentage['WT'].eq(True)]\n",
    "            filtered_adjusted_percentage_2 = pd.concat([filtered_adjusted_percentage_2.copy(), WT_row])\n",
    "        \n",
    "        filtered_adjusted_percentage_2.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_filtered_adj_percentage_allele_frequency_table_around_sgRNA.csv',\n",
    "                index=False) \n",
    "\n",
    "        # Get correlations matrix of log-normalized rpm, using only alleles with > 100 reads in at least one sample\n",
    "        merge = merge[merge['#read_count_filter'] == True]\n",
    "        cols = [x for x in list(merge) if 'lognorm' in x]\n",
    "        correlations = merge[cols].corr(method='pearson')\n",
    "        correlations['R1'] = correlations.index\n",
    "        correlations = correlations.melt(id_vars = 'R1', value_vars=list(correlations).remove('R1'),var_name='R2',value_name='Pearson')\n",
    "        \n",
    "        # Drop correlations that are not specified in corr_input\n",
    "        correlations['Reps'] = correlations.apply(get_corr_name,axis=1)\n",
    "        combos = get_correlation_cols(corr_input,sg)\n",
    "        correlations = correlations[correlations['Reps'].isin(combos)]\n",
    "        \n",
    "        # Drop duplicate rows (i.e. A vs B and B vs A)\n",
    "        correlations = correlations.drop_duplicates(subset=['Reps'])\n",
    "        \n",
    "        # Write to file\n",
    "        Path(output_filepath + \"corr_outputs/#read_count_filter_pearson/\").mkdir(parents=True, exist_ok=True)\n",
    "        correlations.to_csv(output_filepath + \"corr_outputs/#read_count_filter_pearson/sg\" +str(r['sg'])+'_'+r['primer']+'_correlations.csv')\n",
    "#         break\n",
    "    return \n",
    "\n",
    "hide_toggle(toggle_text_addon='run function')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_15303279888815986903() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_15303279888815986903()\">Show/Hide heatmap function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "rcParams['font.family'] = 'monospace' #so translated sequences are aligned\n",
    "rcParams['font.monospace'] = 'Arial'\n",
    "rcParams['font.size']: 12.0\n",
    "\n",
    "'''\n",
    "This function generates allele-level heatmaps using log-fold changes for each validation condtion\n",
    "annotated with the corresponding allele amino acid sequences\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "df : input file with columns 'sg', 'translation_ref_seq', 'sgRNA_sequence', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "vmin : sets minimum value for heatmap color bar\n",
    "vmax: sets maximum value for heatmap color bar\n",
    "filepath: same as output_filepath specified above where allele frequency output tables are saved \n",
    "time1: (optional) string describing early time point for table header, e.g. 'Day7' (default: 'Time1')\n",
    "time2: (optional) string describing late time point for table header, e.g. 'Day14' (default: 'Time2')\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "1. allele-level heatmpas with corresponding allele amino acid sequences annotated\n",
    "2. tables with average % reads for each allele at early and late time points \n",
    "\n",
    "'''\n",
    "\n",
    "def heatmaps(df,vmin,vmax,filepath, time1 = 'Time1', time2 = 'Time2', timepoint_df=None, filename = 'translation_heatmap', **kwargs):\n",
    "\n",
    "    num_rows = len(df)\n",
    "    num_cols = 2\n",
    "    fig,axs = plt.subplots(num_rows, num_cols, figsize=(num_cols*4,num_rows*4))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace = 0.5)\n",
    "    \n",
    "    row_count = 0\n",
    "    \n",
    "    for num, (i,r) in enumerate(df.iterrows()):\n",
    "\n",
    "        sg = r['sg']\n",
    "\n",
    "        primer = r['primer']\n",
    "        \n",
    "        location = filepath+str(sg)+'_'+primer+'_filtered_adj_percentage_allele_frequency_table_around_sgRNA.csv'\n",
    "\n",
    "        to_plot = pd.read_csv(location)\n",
    "\n",
    "        avgLFC_col = [col for col in to_plot.columns if 'AvgLFC' in col][0]\n",
    "\n",
    "        to_plot.sort_values(by=avgLFC_col,ascending=False,inplace=True)\n",
    "        \n",
    "        # Earlier time point -> Time1\n",
    "        time1_avg_col = 'Avg%Reads_' + time1\n",
    "        \n",
    "        # Later time point -> Time 2\n",
    "        time2_avg_col = 'Avg%Reads_' + time2\n",
    "        \n",
    "        if timepoint_df is None: # Default to BEV_ref as Time1 samples and BEV_test as Time2 samples\n",
    "            \n",
    "            # Default Time 1 (Earlier time point) samples  = BEV_ref columns \n",
    "            time1_samples = r['BEV_ref'].split(';') # convert to list of BEV numbers\n",
    "\n",
    "            # Default Time 2 (Later time point) samples  = BEV_ref columns \n",
    "            time2_samples = r['BEV_test'].split(';') # convert to list of BEV numbers \n",
    "            \n",
    "        else: # if timepoint_df provided\n",
    "            # Get Time 1 columns from df based on time1 parameter \n",
    "            sg_timepoint_df = timepoint_df[timepoint_df['sg']==sg]\n",
    "            time1_sample_df = sg_timepoint_df[sg_timepoint_df['time_point']==time1].reset_index(drop=True)\n",
    "            time1_samples = time1_sample_df.loc[0, 'BEV_nums'].split(';')\n",
    "            \n",
    "            # Get Time 2 columns from df based on time2 parameter \n",
    "            sg_timepoint_df = timepoint_df[timepoint_df['sg']==sg]\n",
    "            time2_sample_df = sg_timepoint_df[sg_timepoint_df['time_point']==time2].reset_index(drop=True)\n",
    "            time2_samples = time2_sample_df.loc[0, 'BEV_nums'].split(';')\n",
    "        \n",
    "        # Lists to store correctly formatted %Reads column names \n",
    "        time1_cols = [] \n",
    "        time2_cols = []\n",
    "        \n",
    "        # %Reads columns named \"%Reads_BEV_\" + get_bev_str(int(BEV_num))\n",
    "        for num in time1_samples:\n",
    "            col_name = \"%Reads_BEV_\" + get_bev_str(int(num))\n",
    "            time1_cols.append(col_name)  \n",
    "\n",
    "        to_plot[time1_avg_col] = round(to_plot[time1_cols].apply(np.mean,axis=1),1).apply(str)#+'%'\n",
    "\n",
    "        # %Reads columns named \"%Reads_BEV_\" + get_bev_str(int(BEV_num))\n",
    "        for num in time2_samples:\n",
    "            col_name = \"%Reads_BEV_\" + get_bev_str(int(num))\n",
    "            time2_cols.append(col_name)  \n",
    "\n",
    "        to_plot[time2_avg_col] = round(to_plot[time2_cols].apply(np.mean,axis=1),1).apply(str)#+'%'\n",
    "        \n",
    "        label_list = []\n",
    "        chars_to_remove = ['[', ']', \"'\"]\n",
    "        for idx, row in to_plot.iterrows():\n",
    "            translation_list_str = row['Translated']\n",
    "            label = translation_list_str\n",
    "            for char in chars_to_remove:\n",
    "                label = label.replace(char, '')\n",
    "            # if multiple strings in translation_list_str e.g.['Possible UTR mutation', 'TEEPQSDPSVEPPL']\n",
    "            if ',' in label:\n",
    "                split_label = label.split(', ')\n",
    "                # Only show amino acid translation with + on heatmap to indicate that there are other edits\n",
    "                keep_aa_translation = [trans for trans in split_label if 'Possible' not in trans][0] \n",
    "                label = keep_aa_translation + ' +'\n",
    "            label_list.append(label)\n",
    "        \n",
    "        to_plot['Label'] = label_list\n",
    "\n",
    "        reads_df = to_plot[[time1_avg_col, time2_avg_col]]\n",
    "\n",
    "        to_plot.loc[to_plot['WT'],'Label'] = to_plot.loc[to_plot['WT'],'Label'].values[0] + '<wt' \n",
    "\n",
    "        heatmap_df = to_plot[['Label',[col for col in to_plot.columns if 'AvgLFC' in col][0]]].reset_index(drop = True)\n",
    "        \n",
    "        if row_count < num_rows:\n",
    "            # HEATMAP\n",
    "            if num_rows == 1: \n",
    "                heatmap_ax = axs[0]\n",
    "            else: \n",
    "                heatmap_ax = axs[row_count, 0]\n",
    "            \n",
    "            cmap = LinearSegmentedColormap.from_list(name='test', colors=['#8DA0CB','white','#F8774F'])\n",
    "            sns.heatmap(heatmap_df.set_index('Label'),cmap=cmap,vmin=vmin,vmax=vmax,ax=heatmap_ax,annot=True, yticklabels = True, fmt='.1f')\n",
    "            label_list = [item for item in heatmap_ax.get_yticklabels()]\n",
    "            for label in label_list:\n",
    "                if '<wt' in label.get_text():\n",
    "                    label.set_color('red')\n",
    "            plt.setp(heatmap_ax.get_yticklabels(), rotation=0, ha=\"left\", size = 12)#,rotation_mode=\"anchor\")\n",
    "\n",
    "            yax = heatmap_ax.get_yaxis()\n",
    "            # find the maximum width of the label on the major ticks\n",
    "            pad = max(T.label.get_window_extent().width for T in yax.majorTicks)\n",
    "            yax.set_tick_params(pad=pad)\n",
    "            heatmap_ax.set_ylabel('', rotation = 0)\n",
    "            heatmap_ax.set_title(sg)\n",
    "                    \n",
    "            bbox = heatmap_ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            width, height = bbox.width, bbox.height\n",
    "\n",
    "            # TABLE\n",
    "            cell_text = []\n",
    "            for row in range(len(reads_df)):\n",
    "                cell_text.append(reads_df.iloc[row])\n",
    "            \n",
    "            if num_rows == 1: \n",
    "                table_ax = axs[1]\n",
    "            else: \n",
    "                table_ax = axs[row_count, 1]\n",
    "            #print('table position:', table_ax)\n",
    "            translations = list(heatmap_df['Label'])\n",
    "            #print(translations)\n",
    "            cell_height = height/(2*(len(translations)))\n",
    "\n",
    "            reads_table = table_ax.table(cellText=cell_text, colLabels=reads_df.columns, loc='center', cellLoc = 'center', edges = 'open')#, bbox = [0,0, width, height/2.3])\n",
    "            reads_table.auto_set_font_size(False)\n",
    "            reads_table.set_fontsize(12)\n",
    "            reads_table.auto_set_column_width(col=list(range(len(reads_df.columns))))\n",
    "            #reads_table.scale(1, 1.5)\n",
    "            cellDict = reads_table.get_celld()\n",
    "            \n",
    "            for i in range(0,len(reads_df.columns)):\n",
    "                cellDict[(0,i)].set_height(cell_height)\n",
    "                for j in range(1,len(reads_df)+1):\n",
    "                    cellDict[(j,i)].set_height(cell_height)\n",
    "            table_ax.set_axis_off()\n",
    "            \n",
    "\n",
    "            row_count+=1\n",
    "\n",
    "    # Create path to Figures folder if doesn't exist already\n",
    "    Path(filepath + '/Figures/').mkdir(parents=True, exist_ok=True)\n",
    "    full_filepath = filepath + 'Figures/' + filename +'.pdf'\n",
    "    print(full_filepath)\n",
    "    # Print meaning of +\n",
    "    print('+ indicates that there are additional UTR and/or intronic mutations in this allele. '+\n",
    "          'Please check the output tables for further details.')\n",
    "    plt.savefig(full_filepath,bbox_inches=\"tight\")\n",
    "\n",
    "hide_toggle(toggle_text_addon='heatmap function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "<font color='blue'> Please follow steps indicated in blue, then run the notebook to generate output files. If the files are formatted as described in the documentation, the code in the 'Functions' section should not need to be altered. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metainformation file** \n",
    "\n",
    "<font color='blue'> <b>Step 1:</b> Create metainformation input file in the following format </font> \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **sgRNA_sequence** : sequence of sgRNA as designed \n",
    "* **translation_ref_seq**: reference sequence outputted by CRISPResso formatted such that any intronic sequences are lower-case, exons are upper-case, and UTRs are indicated by square brackets (if applicable) <u> must be sequence on strand that is being translated; may not necessarily be the same strand as the sgRNA sequence</u> \n",
    "    * Ex. <font color='grey'>tgtcttttctatgatctctttag</font><font color='green'>GGGTGACCCAGTCTATT</font>\n",
    "* **primer** : name of primer pair (joined by '\\_') used to amplify genomic locus as mentioned in sample name\n",
    "    * Ex. <font color='purple'>F_C12</font><font color = 'blue'><b>_</b></font><font color='green'>R_C12</font>\n",
    "* **frame** : frame for translation (manually determined for each sg / primer pair); position of first coding nucleotide in reference sequence within codon; frame can be 1, 2, 3\n",
    "    * Ex. given reference sequence: tgtcttttctatgatctctttag<font color='green'>**G**</font>G|GTG|ACC|CAG|TCT|ATT \n",
    "        since the first coding nucleotide of the reference sequence (<font color='green'><b>G</b></font>) is the 2nd nucleotide in its codon \n",
    "        (\\_<font color='green'><b>G</b></font>G) &rightarrow; frame = 2\n",
    "* **first_codon** : first codon for translation \n",
    "* **last_codon** : last codon for translation \n",
    "* **rev_com** : samples for which reference sequence is on reverse strand \n",
    "* **BEV_ref** : reference sample(s) for log-fold change (LFC) calculation (i.e. early time point, empty vector, etc.); if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "* **BEV_test** : test sample(s) for LFC calculation; if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "\n",
    "**Example input:**\n",
    "\n",
    "\n",
    "| sg      | sgRNA_sequence       | translation_ref_seq                                  | primer        | frame | first_codon| last_codon| rev_com | BEV_ref | BEV_test |\n",
    "| ------- | -------------------- | ---------------------------------------- | ------------- |  ----|----|---: | ------: | ------- | -------- |\n",
    "| 397   | GTCACCCCTAAAGAGATCAT | tgtcttttctatgatctctttagGGGTGACCCAGTCTATT |F_C12_R_C12 |  2    |TGG|ATT| True    | 5;6     | 9;10     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> <b> Step 1: </b> Enter filepath to metainformation input file here </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sg</th>\n",
       "      <th>sgRNA_sequence</th>\n",
       "      <th>translation_ref_seq</th>\n",
       "      <th>primer</th>\n",
       "      <th>frame</th>\n",
       "      <th>first_codon</th>\n",
       "      <th>last_codon</th>\n",
       "      <th>rev_com</th>\n",
       "      <th>BEV_ref</th>\n",
       "      <th>BEV_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d</td>\n",
       "      <td>GCTCCTCCATGGCAGTGACC</td>\n",
       "      <td>[TTCCTCTTGCAGCAGCCAGACTGCCTTCCGGGTCACTGCC]ATGGAGGAGCCGCAGTCAGATCCTAGCGTCGAGCCCCCTC</td>\n",
       "      <td>F3_R2</td>\n",
       "      <td>1</td>\n",
       "      <td>ATG</td>\n",
       "      <td>CTG</td>\n",
       "      <td>True</td>\n",
       "      <td>417;418</td>\n",
       "      <td>425;426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sg        sgRNA_sequence  \\\n",
       "0  1d  GCTCCTCCATGGCAGTGACC   \n",
       "\n",
       "                                                                  translation_ref_seq  \\\n",
       "0  [TTCCTCTTGCAGCAGCCAGACTGCCTTCCGGGTCACTGCC]ATGGAGGAGCCGCAGTCAGATCCTAGCGTCGAGCCCCCTC   \n",
       "\n",
       "  primer  frame first_codon last_codon  rev_com  BEV_ref BEV_test  \n",
       "0  F3_R2      1         ATG        CTG     True  417;418  425;426  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_file = pd.read_csv('../../AudreyData/TP53/Metainfo_input_ABE_TP53_fixed_updated_1d_sample.csv')\n",
    "input_filepath = input(\"Please enter input filepath here: \")\n",
    "input_file = pd.read_csv(input_filepath)\n",
    "# input_file = input_file.copy().drop(['BEV_start','BEV_end'], axis=1)\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for NaN values i.e. blank rows\n",
    "if input_file.isnull().values.any(): \n",
    "    input_file = clean_input_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 744.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg: 1d\n",
      "Translation reference sequence: [TTCCTCTTGCAGCAGCCAGACTGCCTTCCGGGTCACTGCC]ATGGAGGAGCCGCAGTCAGATCCTAGCGTCGAGCCCCCTC\n",
      "Translation input: ['ATGGAGGAGCCGCAGTCAGATCCTAGCGTCGAGCCCCCTC']\n",
      "40 0\n",
      "1\n",
      "Expected WT Translation:  ['MEEPQSDPSVEPPL']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Input file is correct!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_input_file(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Metainformation file** \n",
    "\n",
    "<font color='blue'><b>Step 3:</b> Create correlation input file in the following format  </font> \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **reps_for_correlation** : semicolon-separated BEV numbers of which to calculate the pairwise Pearson correlation of the log-normalized read counts\n",
    "\n",
    "**Example input:**\n",
    "    \n",
    "| sg      | reps_for_correlation |\n",
    "| ------- | -------------------: | \n",
    "| 397     | 7;8 | \n",
    "| 397     | 9;10 | \n",
    "| 397     | 11;12 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 4:</b> Enter filepath to correlation input file here  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter correlation input filepath here: /Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/SampleCorrelation_input_TP53_ABE.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sg</th>\n",
       "      <th>reps_for_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d</td>\n",
       "      <td>425;426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sg reps_for_correlation\n",
       "0  1d              425;426"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corr_input = pd.read_csv('../../AudreyData/TP53/SampleCorrelation_input_TP53_ABE.csv')\n",
    "corr_corr_inputpath = input(\"Please enter correlation input filepath here: \")\n",
    "corr_input = pd.read_csv(corr_corr_inputpath)\n",
    "corr_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify folder filepaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 5:</b> Enter filepath to folder containing CRISPResso output files here. Please make sure that the filepath does not begin with a '/' but does end in a '/'.  </font> \n",
    "\n",
    "Please note that each folder containing CRISPResso output files for individual samples within the given folder should be named in the format 'CRISPResso_on_'+bev+'\\_'+\n",
    "primer, where bev = ('BEV' or 'NGBEV') + sample_number and primer = primer name. \n",
    "Ex. <font color='grey'>CRISPResso_on</font><font color='purple'>_BEV_001</font><font color='green'>_F2_R2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter either 'BEV' or 'NGBEV' to indicate which string is used when naming your CRISPResso files.NGBEV\n"
     ]
    }
   ],
   "source": [
    "global bev_string_id\n",
    "bev_string_id = input('Please enter either \\'BEV\\' or \\'NGBEV\\' to indicate which string is used when naming your CRISPResso files.')\n",
    "if ((bev_string_id != 'BEV') and (bev_string_id != 'NGBEV')):\n",
    "    raise Exception('Invalid input. Please enter either \\'BEV\\' or \\'NGBEV\\' to specify which string is used in CRISPResso file names. Be careful not to add any extra spaces.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter CRISPResso filepath here: /Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso/\n"
     ]
    }
   ],
   "source": [
    "global CRISPResso_filepath \n",
    "# CRISPResso_filepath = '../../AudreyData/TP53/TP53_ABE_Sample_CRISPResso/'\n",
    "CRISPResso_filepath = input(\"Please enter CRISPResso filepath here: \")\n",
    "CRISPResso_filepath = check_folder_filepath(CRISPResso_filepath)\n",
    "print(CRISPResso_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 6:</b> Enter filepath to folder where the files generated by this notebook will be stored. Please make sure that the filepath does not begin with a '/' but does end in a '/'. If the folders in this file path do not currently exist, they will be created when the notebook is run.  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter output folder filepath here: /Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/ABE_v2/\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/ABE_v2/\n"
     ]
    }
   ],
   "source": [
    "# Filepath to store allele_freq output tables \n",
    "global output_filepath \n",
    "# output_filepath = '../../AudreyData/TP53/ABE/'\n",
    "output_filepath = input(\"Please enter output folder filepath here: \")\n",
    "output_filepath = check_folder_filepath(output_filepath)\n",
    "print(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the two input files and produce allele tables for all sgRNAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1d\n",
      "[('417', 'GCTCCTCCATGGCAGTGACC', 'F3_R2'), ('418', 'GCTCCTCCATGGCAGTGACC', 'F3_R2'), ('425', 'GCTCCTCCATGGCAGTGACC', 'F3_R2'), ('426', 'GCTCCTCCATGGCAGTGACC', 'F3_R2')]\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso/CRISPResso_on_NGBEV_417_F3_R2/Alleles_frequency_table_around_sgRNA_GCTCCTCCATGGCAGTGACC.txt\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso/CRISPResso_on_NGBEV_418_F3_R2/Alleles_frequency_table_around_sgRNA_GCTCCTCCATGGCAGTGACC.txt\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso/CRISPResso_on_NGBEV_425_F3_R2/Alleles_frequency_table_around_sgRNA_GCTCCTCCATGGCAGTGACC.txt\n",
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/TP53_ABE_Sample_CRISPResso/CRISPResso_on_NGBEV_426_F3_R2/Alleles_frequency_table_around_sgRNA_GCTCCTCCATGGCAGTGACC.txt\n",
      "['%Reads_BEV_417', '%Reads_BEV_418', '%Reads_BEV_425', '%Reads_BEV_426']\n",
      "only UTRs exist\n",
      "num rows in merge df: 10750\n",
      "                                                                      Aligned_Sequence  \\\n",
      "1493  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "1892  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCGTGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2120  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCGTGGCGGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2403  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCTTCCGTGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "\n",
      "                                                                    Reference_Sequence  \\\n",
      "1493  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "1892  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2120  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2403  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "\n",
      "      #Reads_BEV_417  %Reads_BEV_417  #Reads_BEV_418  %Reads_BEV_418  \\\n",
      "1493         25923.0       22.884206         32689.0       20.894883   \n",
      "1892         64999.0       57.379567         92961.0       59.420883   \n",
      "2120          3177.0        2.804580          4751.0        3.036850   \n",
      "2403          2334.0        2.060400          3524.0        2.252549   \n",
      "\n",
      "      #Reads_BEV_425  %Reads_BEV_425  #Reads_BEV_426  %Reads_BEV_426  ...  \\\n",
      "1493         44643.0       53.600759         33536.0       52.083430  ...   \n",
      "1892         21787.0       26.158630         16969.0       26.353880  ...   \n",
      "2120          2572.0        3.088080          2269.0        3.523894  ...   \n",
      "2403          2098.0        2.518970          1757.0        2.728727  ...   \n",
      "\n",
      "      #Reads_BEV_418;lognorm  #Reads_BEV_425;lognorm  #Reads_BEV_426;lognorm  \\\n",
      "1493               17.672797               19.031897               18.990468   \n",
      "1892               19.180613               17.996933               18.007661   \n",
      "2120               14.890336               14.914469               15.104924   \n",
      "2403               14.459335               14.620604               14.735993   \n",
      "\n",
      "      %read_count_filter  #read_count_filter  \\\n",
      "1493                True                True   \n",
      "1892                True                True   \n",
      "2120                True                True   \n",
      "2403                True                True   \n",
      "\n",
      "                                   Translated     WT  LFC_425-417  \\\n",
      "1493                         [MEEPQSDPSVEPPL]   True     1.227898   \n",
      "1892                         [TEEPQSDPSVEPPL]  False    -1.133247   \n",
      "2120  [Possible UTR mutation, TEEPQSDPSVEPPL]  False     0.138921   \n",
      "2403                         [TEEPQSDPSVEPPL]  False     0.289897   \n",
      "\n",
      "      LFC_426-418  AvgLFC_425_426  \n",
      "1493     1.317671        1.272784  \n",
      "1892    -1.172952       -1.153099  \n",
      "2120     0.214588        0.176754  \n",
      "2403     0.276658        0.283278  \n",
      "\n",
      "[4 rows x 21 columns]\n",
      "96433.0\n",
      "133925.0\n",
      "71100.0\n",
      "54531.0\n",
      "percent condition %Reads_BEV_417    False\n",
      "%Reads_BEV_418    False\n",
      "%Reads_BEV_425    False\n",
      "%Reads_BEV_426    False\n",
      "Name: 1493, dtype: bool\n",
      "percent condition %Reads_BEV_417    False\n",
      "%Reads_BEV_418    False\n",
      "%Reads_BEV_425    False\n",
      "%Reads_BEV_426    False\n",
      "Name: 1892, dtype: bool\n",
      "percent condition %Reads_BEV_417    False\n",
      "%Reads_BEV_418    False\n",
      "%Reads_BEV_425    False\n",
      "%Reads_BEV_426    False\n",
      "Name: 2120, dtype: bool\n",
      "percent condition %Reads_BEV_417    False\n",
      "%Reads_BEV_418    False\n",
      "%Reads_BEV_425    False\n",
      "%Reads_BEV_426    False\n",
      "Name: 2403, dtype: bool\n",
      "                                                                      Aligned_Sequence  \\\n",
      "1493  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "1892  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCGTGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2120  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCGTGGCGGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2403  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCTTCCGTGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "\n",
      "                                                                    Reference_Sequence  \\\n",
      "1493  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "1892  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2120  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "2403  GAGGGGGCTCGACGCTAGGATCTGACTGCGGCTCCTCCATGGCAGTGACCCGGAAGGCAGTCTGGCTGCTGCAAGAGGAA   \n",
      "\n",
      "      #Reads_BEV_417  %Reads_BEV_417  #Reads_BEV_418  %Reads_BEV_418  \\\n",
      "1493         25923.0       26.881877         32689.0       24.408438   \n",
      "1892         64999.0       67.403275         92961.0       69.412731   \n",
      "2120          3177.0        3.294515          4751.0        3.547508   \n",
      "2403          2334.0        2.420333          3524.0        2.631324   \n",
      "\n",
      "      #Reads_BEV_425  %Reads_BEV_425  #Reads_BEV_426  %Reads_BEV_426  ...  \\\n",
      "1493         44643.0       62.789030         33536.0       61.498964  ...   \n",
      "1892         21787.0       30.642757         16969.0       31.118080  ...   \n",
      "2120          2572.0        3.617440          2269.0        4.160936  ...   \n",
      "2403          2098.0        2.950774          1757.0        3.222021  ...   \n",
      "\n",
      "      #Reads_BEV_418;lognorm  #Reads_BEV_425;lognorm  #Reads_BEV_426;lognorm  \\\n",
      "1493               17.672797               19.031897               18.990468   \n",
      "1892               19.180613               17.996933               18.007661   \n",
      "2120               14.890336               14.914469               15.104924   \n",
      "2403               14.459335               14.620604               14.735993   \n",
      "\n",
      "      %read_count_filter  #read_count_filter  \\\n",
      "1493                True                True   \n",
      "1892                True                True   \n",
      "2120                True                True   \n",
      "2403                True                True   \n",
      "\n",
      "                                   Translated     WT  LFC_425-417  \\\n",
      "1493                         [MEEPQSDPSVEPPL]   True     1.227898   \n",
      "1892                         [TEEPQSDPSVEPPL]  False    -1.133247   \n",
      "2120  [Possible UTR mutation, TEEPQSDPSVEPPL]  False     0.138921   \n",
      "2403                         [TEEPQSDPSVEPPL]  False     0.289897   \n",
      "\n",
      "      LFC_426-418  AvgLFC_425_426  \n",
      "1493     1.317671        1.272784  \n",
      "1892    -1.172952       -1.153099  \n",
      "2120     0.214588        0.176754  \n",
      "2403     0.276658        0.283278  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "run(input_file,corr_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate allele-level heat maps annotated with corresponding allele amino acid sequences and tables with average % reads for each allele at early and late time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, early time point samples are considered to be the same as the BEV_ref samples and the late time point samples are considered to be the BEV_test samples. If this is not the case (e.g.,  BEV_test samples span multiple time points), please upload a file with the following columns: \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **Time point** : string that identifies time point \n",
    "* **BEV_num** : semicolon-separated BEV numbers corresponding to samples at that time point \n",
    "\n",
    "**Example input:**\n",
    "    \n",
    "| sg      |time_point | BEV_nums |\n",
    "| ------- |------- | -------------------: | \n",
    "| 397     |D8      |  7;8 | \n",
    "| 397     |D14      | 9;10 | \n",
    "| 397     |D21      | 11;12 | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the default time point sample assignments work for you? Please enter 'y' or 'n'. If 'n', you will be asked to enter the path to a time point input file as described above. y\n"
     ]
    }
   ],
   "source": [
    "global timepoint_input_file\n",
    "\n",
    "# Check if default settings work for user\n",
    "default_y_or_n = input(\"Do the default time point sample assignments work for you? Please enter 'y' or 'n'. If 'n', you will be asked to enter the path to a time point input file as described above. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_y_or_n = default_y_or_n.lower()\n",
    "if default_y_or_n == 'y':\n",
    "    timepoint_input_file = None\n",
    "#     print('y')\n",
    "elif default_y_or_n == 'n':\n",
    "    timepoint_input_filepath = input(\"Please enter input filepath here: \")\n",
    "    # timepoint_input_file = pd.read_csv('AnnabelData/timepoint_df_test.csv')\n",
    "    timepoint_input_file = pd.read_csv(timepoint_input_filepath)\n",
    "#     print('n')\n",
    "else:\n",
    "    raise Exception('Invalid input. Please enter either \\'y\\' or \\'n\\' and re-run the cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/GoogleDrive/Shared drives/GPP Cloud /R&D/People/Priyanka/BaseEditorValidation/AudreyData/TP53/ABE_v2/Figures/TP53_ABE_heatmap_1d.pdf\n",
      "+ indicates that there are additional UTR and/or intronic mutations in this allele. Please check the output tables for further details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmaps(df = input_file,\n",
    "         vmin = -2,\n",
    "         vmax = 2,\n",
    "         filepath = output_filepath,\n",
    "         time1 = 'D7',\n",
    "         time2 = 'D21',\n",
    "         timepoint_df = timepoint_input_file,\n",
    "         filename = 'TP53_ABE_heatmap_1d'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}