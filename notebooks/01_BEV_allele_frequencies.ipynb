{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making allele frequency plots\n",
    "\n",
    "This notebook contains code to process the CRISPResso \"allele frequency table\" files from base editor validation experiments. The inputs are 2 input files: the first contains metainformation about each sample to make the \"allele frequency\" file, and the second contains metainformation to compute correlations between the log-normalized read counts. The output of this file are 3 files for each sgRNA / primer pair: \n",
    "1. a file containing all alleles and their read counts for each sample\n",
    "2. a filtered version of (1) that only contains alleles with at least 1% abundance in any sample\n",
    "3. a file containing the Pearson correlations between log-normalized read counts of each allele with > 100 reads in at least one sample\n",
    "\n",
    "(1) is the starting file used to show the abundance of specific edits over time (code in BEV_aa_over_time.ipynb). (2) is the starting file used to create allele-level heatmaps (this was done using GraphPad Prism). (3) is the starting file for the plots showing the replicate correlation for validation experiments (actual plots were made using GraphPad Prism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_17152516152839775688() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_17152516152839775688()\">Show/Hide Toggle Function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(toggle_text_addon = '', for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "    \n",
    "    toggle_text = 'Show/Hide' + ' ' + toggle_text_addon  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "hide_toggle(toggle_text_addon='Toggle Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import core_functions as cfs\n",
    "from math import log\n",
    "from os import path\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "print('Python version: ' + sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.2.1\n",
      "numpy 1.19.5\n"
     ]
    }
   ],
   "source": [
    "modules = ['pandas', 'numpy']\n",
    "for module in modules:\n",
    "    try:\n",
    "        print(module + ' ' + sys.modules[module].__version__)\n",
    "    except:\n",
    "        print(module + ' has no __version__ attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_8858957115724279543() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_8858957115724279543()\">Show/Hide clean_input_file function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This function removes any NaN rows from input_file\n",
    "'''\n",
    "def clean_input_file(df):\n",
    "    df = df.dropna() #drop NaN \n",
    "    #dropna() converts int to float, so convert them back\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns #select subset of df of type float\n",
    "    for col in float_cols: \n",
    "        #get original column index so can replace at correct loc\n",
    "        index = df.columns.get_loc(col)\n",
    "        #rename float cols as \"float_\"col name \n",
    "        float_col_name = 'float_' + col\n",
    "        df = df.rename(columns = {col : float_col_name})\n",
    "        #overwrite as type int\n",
    "        float_to_int = df[float_col_name].astype(int).copy()\n",
    "        df.insert(index, col, float_to_int)\n",
    "        #drop float column \n",
    "        df = df.drop(float_col_name, axis = 1)\n",
    "    return df\n",
    "\n",
    "hide_toggle(toggle_text_addon='clean_input_file function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_16169348129179121576() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_16169348129179121576()\">Show/Hide remove_utr function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes UTRs from Aligned_Sequence before translation\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "translation_ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without UTRs and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "def remove_utr(translation_ref_seq, aligned, rev_com):\n",
    "    \n",
    "    if ('[' not in translation_ref_seq) or (']' not in translation_ref_seq): # if no UTR\n",
    "        return\n",
    "    \n",
    "    open_bracket = False # True when encounters open bracket [\n",
    "    closing_bracket = False # True when encounters closed bracket ]\n",
    "    in_utr = False # True when open_bracket == True and closing_bracket == False\n",
    "    utr_list = []\n",
    "    utr = ''\n",
    "    utr_start_end_list = []\n",
    "    start_end = [] \n",
    "    bracket_count = 0\n",
    "    # check to make sure all open brackets are only followed by closed, and closed is never followed by closed\n",
    "    for idx, char in enumerate(translation_ref_seq):\n",
    "        if char == '[':\n",
    "            if open_bracket: # if already encountered open bracket\n",
    "                return 'Open bracket encountered after open bracket. Please make sure all brackets are closed.'\n",
    "            else:\n",
    "                open_bracket = True\n",
    "                closing_bracket = False\n",
    "                \n",
    "        if char == ']':\n",
    "            if open_bracket: # if this ] closes encountered open bracket\n",
    "                open_bracket = False\n",
    "                closing_bracket = True\n",
    "                bracket_count +=1 # count closed bracket \n",
    "                in_utr = False # end of UTR\n",
    "                utr_list.append(utr) # add UTR sequence to list\n",
    "                start_end.append(idx-bracket_count) # subtract # brackets encountered\n",
    "                utr_start_end_list.append(start_end)\n",
    "                utr = '' # Reset UTR\n",
    "                start_end = [] # Rest start_end index pair\n",
    "                \n",
    "            elif closing_bracket: # if already encountered closing bracket\n",
    "                return 'Closing bracket encountered after closing bracket. Please check input.'\n",
    "            else:\n",
    "                return 'No open bracket encountered. Please check input.'\n",
    "        \n",
    "        if (open_bracket and not closing_bracket) and (char != '[') and (char != ']'):\n",
    "            in_utr = True \n",
    "            # if first character in utr, store start index\n",
    "            if len(utr) == 0:\n",
    "                bracket_count +=1 # count open bracket\n",
    "                start_end.append(idx-bracket_count) # subtract # brackets encountered\n",
    "            utr = utr + char\n",
    "    \n",
    "#     print(utr_list, utr_start_end_list)\n",
    "    \n",
    "    new_aligned = aligned \n",
    "    new_utrs = []\n",
    "    new_utr_start_end_list = []\n",
    "    if rev_com: # need to reverse complement UTR from translation_ref_seq to match aligned seq\n",
    "        # Reverse UTR index start, end positions if rev_com = True\n",
    "        for i in utr_start_end_list:\n",
    "            new_idx_start = -(i[1]+1) # reverse index, +1 b/c starts at -1 in reverse\n",
    "            # convert to positive index\n",
    "            new_idx_start = len(translation_ref_seq) + new_idx_start\n",
    "            new_idx_end = -(i[0]+1) # reverse index, +1 b/c starts at -1 in reverse\n",
    "            # convert to positive index\n",
    "            new_idx_end = len(translation_ref_seq) + new_idx_end\n",
    "            new_utr_start_end_list.append([new_idx_start, new_idx_end])\n",
    "#       \n",
    "        for utr in utr_list:\n",
    "            rev_com_utr = cfs.revcom(utr)\n",
    "            new_utrs.append(rev_com_utr)\n",
    "    else:\n",
    "        new_utr_start_end_list = utr_start_end_list\n",
    "        new_utrs = utr_list\n",
    "    \n",
    "#     print(utr_start_end_list, new_utr_start_end_list)\n",
    "#     print(utr_list, new_utrs)\n",
    "    len_prev_utr = 0\n",
    "    for idx, utr in enumerate(new_utrs):\n",
    "        # if not first UTR, adjust start and end index based on length of prev UTR\n",
    "        start_end_idx = new_utr_start_end_list[idx]\n",
    "        if start_end_idx[0] < 0: # if negative index, add len_prev_utr\n",
    "            start_end_idx = [start_end_idx[0]+len_prev_utr, start_end_idx[1]+len_prev_utr]\n",
    "        else:\n",
    "            start_end_idx = [start_end_idx[0]-len_prev_utr, start_end_idx[1]-len_prev_utr]\n",
    "#         print(idx, start_end_idx, utr)\n",
    "        # Check if bases at intron positions in aligned seq match utr\n",
    "        new_aligned_intron = new_aligned[start_end_idx[0]:start_end_idx[1]+1]\n",
    "        if new_aligned_intron != utr.upper():\n",
    "#             print(new_aligned_intron)\n",
    "#             print('not found')\n",
    "            new_aligned = 'UTR'\n",
    "            return new_aligned\n",
    "        else:\n",
    "#             print('found')\n",
    "            # remove intron based on index position \n",
    "            if start_end_idx[0] == 0: # if intron at the beginning of new_aligned seq, keep substring after end_index\n",
    "                new_aligned = new_aligned[start_end_idx[1]+1:]\n",
    "            else: # (bases until start index + bases after end index)\n",
    "                new_aligned = new_aligned[:start_end_idx[0]]+new_aligned[start_end_idx[1]+2:]\n",
    "#             print(new_aligned)\n",
    "            len_prev_utr = len(utr)+1 # adjust for zero index\n",
    "        \n",
    "    if rev_com: # Now that UTR has been found, reverse complement again so correct translation input\n",
    "        new_aligned = cfs.revcom(new_aligned)\n",
    "# #     print(new_aligned)\n",
    "    return new_aligned\n",
    "\n",
    "hide_toggle(toggle_text_addon='remove_utr function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_9545194880013664751() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_9545194880013664751()\">Show/Hide remove_introns function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes introns from Aligned_Sequence before translation\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "translation_ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without introns and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "def remove_introns(translation_ref_seq, aligned, rev_com):\n",
    "    # get index positions of introns \n",
    "    intron_idx= [i for i, base in enumerate(translation_ref_seq) if base.islower()]\n",
    "    intron_idx_list = []\n",
    "    # Get consecutive indices to group indices for same intron\n",
    "    for k, g in groupby(enumerate(intron_idx), lambda ix : ix[0] - ix[1]):\n",
    "         intron_idx_list.append(list(map(itemgetter(1), g)))\n",
    "    \n",
    "    # if no introns, exit function\n",
    "    if not intron_idx_list:\n",
    "        #print('no introns')\n",
    "        return \n",
    "    \n",
    "    # Get sequence of intron to check for intronic mutations \n",
    "    introns = []\n",
    "    intron_start_end_list = []\n",
    "    for intron_idx_group in intron_idx_list:\n",
    "        intron_start = intron_idx_group[0]\n",
    "        intron_end = intron_idx_group[-1]\n",
    "        # store intron start and end index positions in pairs in list \n",
    "        intron_start_end_list.append([intron_idx_group[0], intron_idx_group[-1]])\n",
    "#         print(intron_start_end_list)\n",
    "        intron = translation_ref_seq[intron_start:intron_end+1]\n",
    "        introns.append(intron)\n",
    "        print(introns)\n",
    "    \n",
    "    new_aligned = aligned.upper()\n",
    "    \n",
    "    new_intron_start_end_list = []\n",
    "    new_introns = []    \n",
    "\n",
    "    if rev_com:\n",
    "        # Reverse intron index positions if rev_com = True\n",
    "        for i in intron_start_end_list:\n",
    "            new_idx_start = -(i[1]+1) # reverse index\n",
    "            # convert to positive index\n",
    "            new_idx_start = len(translation_ref_seq) + new_idx_start\n",
    "            new_idx_end = -(i[0]+1) # reverse index\n",
    "            # convert to positive index\n",
    "            new_idx_end = len(translation_ref_seq) + new_idx_end\n",
    "            new_intron_start_end_list.append([new_idx_start, new_idx_end])\n",
    "#             print(i, new_intron_idx_list)\n",
    "        for i in introns:\n",
    "            rev_com_intron = cfs.revcom(i.upper())\n",
    "            new_introns.append(rev_com_intron)\n",
    "#             print(new_introns)\n",
    "    else:\n",
    "        new_intron_start_end_list = intron_start_end_list\n",
    "        new_introns = introns\n",
    "    \n",
    "    print(new_intron_start_end_list)\n",
    "    print(new_introns, new_aligned)\n",
    "    len_prev_intron = 0\n",
    "    for idx, intron in enumerate(new_introns):\n",
    "        # if not first intron, adjust start and end index based on length of prev intron\n",
    "        start_end_idx = new_intron_start_end_list[idx]\n",
    "        if start_end_idx[0] < 0: # if negative index, add len_prev_utr\n",
    "            start_end_idx = [start_end_idx[0]+len_prev_intron, start_end_idx[1]+len_prev_intron]\n",
    "        else:\n",
    "            start_end_idx = [start_end_idx[0]-len_prev_intron, start_end_idx[1]-len_prev_intron]\n",
    "#         print(idx, start_end_idx, intron)\n",
    "        # Check if bases at intron positions in aligned seq match intron\n",
    "        if new_aligned[start_end_idx[0]:start_end_idx[1]+1] != intron.upper():\n",
    "#             print('not found')\n",
    "            new_aligned = 'intron'\n",
    "            return new_aligned\n",
    "        else:\n",
    "#             print('found')\n",
    "            # remove intron based on index position \n",
    "            if start_end_idx[0] == 0: # if intron at the beginning of new_aligned seq, keep substring after end_index\n",
    "                new_aligned = new_aligned[start_end_idx[1]+1:]\n",
    "#                 print(new_aligned)\n",
    "            else: # (bases until intron start index + bases after intron end index)\n",
    "                new_aligned = new_aligned[:start_end_idx[0]]+new_aligned[start_end_idx[1]+1:]\n",
    "#             print(new_aligned)\n",
    "            len_prev_intron = len(intron)+1 # adjust for zero index\n",
    "#     print(new_aligned)\n",
    "    if rev_com:\n",
    "        new_aligned = cfs.revcom(new_aligned)\n",
    "#     print('introns removed, new aligned:', new_aligned)\n",
    "    return new_aligned\n",
    "\n",
    "hide_toggle(toggle_text_addon='remove_introns function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_15456688281782016781() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_15456688281782016781()\">Show/Hide translate function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function returns the translation of a given sequence and frame\n",
    "def translate(seq, frame, first_codon, last_codon, codon_map):\n",
    "#     if not seq: # if remove_introns returned False -> possible splice site mutation\n",
    "    if seq == 'UTR':\n",
    "        return 'Possible UTR mutation'\n",
    "    if seq == 'intron':\n",
    "        return 'Possible intronic mutation'\n",
    "    aa = ''\n",
    "    i = 0\n",
    "    # if frame is not 1, add nucleotides necessary to beginning of sequence for translation based on first_codon\n",
    "    if frame == 2:\n",
    "        seq = first_codon[0] + seq\n",
    "        frame = 1\n",
    "    if frame == 3:\n",
    "        seq = first_codon[0:2] + seq\n",
    "        frame = 1\n",
    "\n",
    "#     print(seq)    \n",
    "    while i < len(seq):\n",
    "        substring = ''\n",
    "        while frame <= 3:\n",
    "            if i<len(seq):\n",
    "                if seq[i] == '-':\n",
    "                    #print('deletion')\n",
    "                    i += 1\n",
    "                    # frame doesn't change\n",
    "                else:\n",
    "                    substring += seq[i]  \n",
    "                    i += 1\n",
    "                    frame+=1\n",
    "            else: # if reached end of the sequence and frame still <=3, complete codon sequence based on last_codon\n",
    "                substring += last_codon[frame-1]\n",
    "                i += 1\n",
    "                frame+=1\n",
    "    \n",
    "#             print('substring:', substring, 'i = ', i, 'frame = ', frame)\n",
    "        if len(substring) == 3:\n",
    "            frame = 1 # reset frame \n",
    "            if ('N' in substring):\n",
    "                aa = aa + '-'\n",
    "            else:\n",
    "                aa = aa + codon_map[substring] # translate codon\n",
    "#             print(aa)\n",
    "        else:\n",
    "            frame = 1\n",
    "#         print(frame)\n",
    "    return aa\n",
    "\n",
    "codon_map = {'TTT':'F', 'TTC':'F', 'TTA':'L', 'TTG':'L', 'CTT':'L', 'CTC':'L', 'CTA':'L', 'CTG':'L', 'ATT':'I', 'ATC':'I',\n",
    "             'ATA':'I', 'ATG':'M', 'GTT':'V', 'GTC':'V', 'GTA':'V', 'GTG':'V', 'TCT':'S', 'TCC':'S', 'TCA':'S', 'TCG':'S',\n",
    "             'CCT':'P', 'CCC':'P', 'CCA':'P', 'CCG':'P', 'ACT':'T', 'ACC':'T', 'ACA':'T', 'ACG':'T', 'GCT':'A', 'GCC':'A',\n",
    "             'GCA':'A', 'GCG':'A', 'TAT':'Y', 'TAC':'Y', 'TAA':'*', 'TAG':'*', 'CAT':'H', 'CAC':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "             'AAT':'N', 'AAC':'N', 'AAA':'K', 'AAG':'K', 'GAT':'D', 'GAC':'D', 'GAA':'E', 'GAG':'E', 'TGT':'C', 'TGC':'C',\n",
    "             'TGA':'*', 'TGG':'W', 'CGT':'R', 'CGC':'R', 'CGA':'R', 'CGG':'R', 'AGT':'S', 'AGC':'S', 'AGA':'R', 'AGG':'R',\n",
    "             'GGT':'G', 'GGC':'G', 'GGA':'G', 'GGG':'G'}\n",
    "\n",
    "hide_toggle(toggle_text_addon='translate function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_5447802723052074784() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_5447802723052074784()\">Show/Hide check_input_file function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_input_file(input_file):\n",
    "    \n",
    "    print('Checking input file...')\n",
    "    # Check each input column for correct formatting\n",
    "\n",
    "    err_flag = False\n",
    "\n",
    "    for i, row in tqdm(input_file.iterrows()):\n",
    "        translation_err_flag = False\n",
    "        utr_flag = False\n",
    "        intron_flag = False\n",
    "        sg = row['sg']\n",
    "        print('sg:', sg)\n",
    "        # sgRNA_sequence must be all caps\n",
    "        input_file.loc[i, 'sgRNA_sequence'] = row['sgRNA_sequence'].upper()\n",
    "        # translation_ref_seq: use WT aligned condition to check formatting, frame, first codon, last codon\n",
    "        ref_seq = row['translation_ref_seq']\n",
    "        print('Translation reference sequence:', ref_seq)\n",
    "        WT_aligned_seq = ref_seq\n",
    "        brackets = '[]'\n",
    "        # if UTR in translation_ref_seq\n",
    "        if ('[' in ref_seq) or (']' in ref_seq):\n",
    "            utr_flag = True\n",
    "            for bracket in brackets:\n",
    "                WT_aligned_seq = WT_aligned_seq.replace(bracket, '')\n",
    "        if any(c.islower() for c in ref_seq):\n",
    "            intron_flag = True\n",
    "            WT_aligned_seq = WT_aligned_seq.upper()\n",
    "        \n",
    "        \n",
    "        translation_input = WT_aligned_seq\n",
    "\n",
    "        # Remove UTRs and/or introns if applicable to check frame, first codon, and last codon \n",
    "        # Since translation_ref_seq in correct orientation for translation, rev_com = False for this check\n",
    "        rev_com = False\n",
    "        if utr_flag:\n",
    "            translation_input = remove_utr(ref_seq, translation_input, rev_com)\n",
    "        if intron_flag:\n",
    "#             print(ref_seq, translation_input, rev_com)\n",
    "            translation_input = remove_introns(ref_seq, translation_input, rev_com)\n",
    "        \n",
    "        # Match frame and translation_input with first codon to check if they match\n",
    "        frame = row['frame']\n",
    "        first_codon = row['first_codon']\n",
    "        last_codon = row['last_codon']\n",
    "        translation_start_idx = frame - 1\n",
    "        print('Translation input:', translation_input)\n",
    "        # Check if first base in translation_input matches corresponding nucleotide in first codon based on frame \n",
    "    #     print(frame, translation_input[0], first_codon[translation_start_idx])\n",
    "        if translation_input[0] != first_codon[translation_start_idx]:\n",
    "            print(translation_input[0], first_codon[translation_start_idx])\n",
    "            print('Frame does not match base in first_codon. Please check inputs') \n",
    "            err_flag = True\n",
    "#             translation_err_flag = \n",
    "        else:\n",
    "            # Check if last codon in translation_ref_seq is complete\n",
    "            # length of translation_input - # bases in start codon present in translation_ref_seq\n",
    "            print(len(translation_input), translation_start_idx)\n",
    "            len_last_codon = (len(translation_input) - (3-translation_start_idx))%3 \n",
    "#             len_last_codon = (len(translation_input) - frame)%3 \n",
    "            print(len_last_codon)\n",
    "    #         print(len_last_codon, translation_input[-len_last_codon:])\n",
    "            if len_last_codon == 0:\n",
    "                if translation_input[-3:] != last_codon:\n",
    "                    print(translation_input[-3:], last_codon)\n",
    "                    print('Input for last_codon does not align with translation_ref_seq. Please check inputs.')\n",
    "                    err_flag = True\n",
    "            else:\n",
    "#                 print(translation_input, translation_input[-len_last_codon:], last_codon)\n",
    "                if translation_input[-len_last_codon:] != last_codon[:len_last_codon]:\n",
    "                    print(translation_input[-len_last_codon:], last_codon[:len_last_codon])\n",
    "                    print('Input for last_codon does not align with frame and translation_ref_seq. Please check inputs.')\n",
    "                    err_flag = True\n",
    "\n",
    "            if not err_flag:\n",
    "                print('Expected WT Translation: ', translate(translation_input, frame, first_codon, last_codon, codon_map))\n",
    "#         break\n",
    "    if not err_flag:\n",
    "        return('Input file is correct!')\n",
    "    else:\n",
    "        return('Please address errors listed above before proceeding.')\n",
    "\n",
    "hide_toggle(toggle_text_addon='check_input_file function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_6957969496433580859() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_6957969496433580859()\">Show/Hide process data functions</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: converts the BEV number from an int to a 3-digit string\n",
    "'''\n",
    "def get_bev_str(bev):\n",
    "    bev = int(bev)\n",
    "    if bev < 10:\n",
    "        return '00'+str(bev)\n",
    "    if bev < 100:\n",
    "        return '0'+str(bev)\n",
    "    return str(bev)\n",
    "'''\n",
    "Called from: get_path\n",
    "Function: Verifies existence of filepath generated by get_path function to retrieve 'Alleles_frequency_table_around_sgRNA' file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "def check_filepath(filepath,bev,primer,sg_seq):\n",
    "    file_loc = filepath+'CRISPResso_on_'+bev+'_'+primer+'/'+'Alleles_frequency_table_around_sgRNA_'+sg_seq+'.txt'\n",
    "    print(file_loc)\n",
    "    if path.exists(file_loc):\n",
    "        return file_loc\n",
    "    else:\n",
    "        raise ValueError('File not found. Please check your filepath inputs.')\n",
    "        return ''\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: calls check_filepath to get filepath where CRISPResso output files are stored, \n",
    "whose folder path (CRISPResso_filepath) is provided above\n",
    "Calls: check_filepath\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_num : BEV sample number\n",
    "primer: primer name (from input file)\n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "\n",
    "def get_path(bev_num,primer,sg_seq):\n",
    "    bev = bev_string_id + '_' + get_bev_str(bev_num)\n",
    "    filepath = CRISPResso_filepath\n",
    "    return check_filepath(filepath,bev,primer,sg_seq)\n",
    "\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: merges together the \"Allele_frequency_table_around_sgRNA\" files\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "existing_df: merge data frame with 'Aligned_Sequence' and 'Reference_Sequence' columns defined in get_bev_files\n",
    "cols: empty list populated in function \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with Aligned_Sequence, Reference_Sequence, Reads columns from each sample \n",
    "cols: columns labeled with BEV sample number \n",
    "'''\n",
    "def merge_bev_file(filepath,bev,sg_seq,existing_df,cols):\n",
    "    if not path.exists(filepath):\n",
    "        print('no file')\n",
    "        return existing_df,cols\n",
    "    df = pd.read_table(filepath,index_col=False)\n",
    "    # Sum together any rows that share both 'Aligned Sequence' and 'Reference Sequence' with each other (this is rare)\n",
    "    df_summed = df[['Aligned_Sequence','Reference_Sequence','#Reads','%Reads']].groupby(['Aligned_Sequence','Reference_Sequence'],as_index=False).agg('sum')\n",
    "    cols.append(str('_BEV_'+str(bev)))\n",
    "    df_summed = df_summed.rename(columns={'#Reads':str('#Reads_BEV_'+str(bev)),'%Reads':str('%Reads_BEV_'+str(bev))})\n",
    "    # Outer merge onto existing dataframe\n",
    "    merge = pd.merge(existing_df,df_summed,how='outer',on=['Aligned_Sequence','Reference_Sequence'])\n",
    "    # Fill in nans with 0\n",
    "    merge = merge.fillna(0)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: function gets and merges all \"Allele_frequency_table_around_sgRNA\" files for a given sgRNA sequence\n",
    "(i.e. different replicates, drug conditions, etc.)\n",
    "This is customized to work with files with the \"BEV\" notation\n",
    "Calls: get_path, merge_bev_file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_list : contains BEV sample number, primer name, guide sequence for each sample  \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge: merged dataframe with \n",
    "cols:\n",
    "\n",
    "'''\n",
    "\n",
    "def get_bev_files(bev_list):\n",
    "    merge = pd.DataFrame(columns=['Aligned_Sequence','Reference_Sequence'])\n",
    "    cols = []\n",
    "    for bev,sg_seq,primer_name in bev_list:\n",
    "        filepath = get_path(bev,primer_name,sg_seq)\n",
    "        if filepath != '':\n",
    "            merge,cols = merge_bev_file(filepath=filepath,bev=bev,sg_seq=sg_seq,existing_df=merge,cols=cols)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: filters out rows that don't meet given threshold (<1% for %Reads and <100 for #Reads)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row in column to which function is being applied (%Reads or #Reads)\n",
    "cols: given cols (%Reads, #Reads for all samples )\n",
    "val: threshold for filter \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns False for any rows (alleles) that have a value < the given value in ALL of the given cols\n",
    "\n",
    "'''\n",
    "def read_count_filter(row,cols,val):\n",
    "    for col in cols:\n",
    "        if row[col] > val:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "'''\n",
    "This function checks for WT allele (if Aligned_Sequence = Reference_Sequence)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of merge dataframe \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns True if the allele is unedited (i.e. WT) and False otherwise\n",
    "\n",
    "'''\n",
    "def get_wt_col(row):\n",
    "    if row['Aligned_Sequence'] == row['Reference_Sequence']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''\n",
    "This function calculates the LFC\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of metainformation input file containing BEV_test and BEV_ref columns\n",
    "data_file : merged dataframe containing log-normalized rpm for each allele\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "data_file : merged dataframe, now with LFC columns for each BEV test / ref pair\n",
    "\n",
    "'''\n",
    "def get_lfc_v2(row,data_file):\n",
    "    cols = []\n",
    "    bev_list = row['BEV_test'].split(';')\n",
    "    \n",
    "    # Go through each test sample in BEV_test column\n",
    "    for i,bev in enumerate(bev_list):\n",
    "        test = get_bev_str(bev)\n",
    "        \n",
    "        # Get reference sample for LFC from BEV_ref column\n",
    "        ref = get_bev_str(row['BEV_ref'].split(';')[i])\n",
    "        \n",
    "        # Calculate LFC\n",
    "        data_file['LFC_'+test+'-'+ref] = data_file['#Reads_BEV_'+test+';lognorm'] - data_file['#Reads_BEV_'+ref+';lognorm']  \n",
    "        cols.append('LFC_'+test+'-'+ref)\n",
    "        \n",
    "    # Average together LFC columns\n",
    "    data_file['AvgLFC_'+'_'.join(bev_list)] = data_file.loc[:,cols].mean(axis=1)\n",
    "    return data_file\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: merges read counts (filtered), lognorms, aligned sequences, reference sequences, translations from \n",
    "test and reference samples for sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "data : deduplicated input file with column 'sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with filtered read counts, lognorms, aligned sequences, reference sequences, translations\n",
    "\n",
    "'''\n",
    "        \n",
    "def process_data_v2(data):\n",
    "    bev_list = [] # list to store info for retrieving CRISPResso files for given sgRNA\n",
    "    ref_nums = [] # list to store BEV numbers for CRISPResso files for reference samples \n",
    "    for i,row in data.iterrows():\n",
    "        for bev in range(row['BEV_start'],row['BEV_end']+1):\n",
    "            # store BEV number, sgRNA sequence, and primer name to get corresponding CRISPResso files \n",
    "            bev_list.append((get_bev_str(bev),row['sgRNA_sequence'],row['primer']))\n",
    "        \n",
    "        # if reference samples outside of BEV_start and BEV_end range, add to bev_list separately\n",
    "        BEV_ref = data['BEV_ref'].copy().tolist() #.copy.loc['BEV_ref']))\n",
    "        BEV_ref_split = BEV_ref[0].split(';')\n",
    "        for num in BEV_ref_split:\n",
    "            # check if reference sample numbers outside of BEV_start and BEV_end range\n",
    "            if ((int(num) < row['BEV_start']) | (int(num) > row['BEV_end'])):\n",
    "                ref_nums.append(num)\n",
    "            else:\n",
    "                continue\n",
    "        for ref_num in ref_nums:\n",
    "            # store BEV number, sgRNA sequence, and primer name to get corresponding CRISPResso files \n",
    "            bev_list.append((get_bev_str(ref_num),row['sgRNA_sequence'],row['primer']))\n",
    "        print(bev_list)\n",
    "    \n",
    "    # Call get_bev_files function which merges Alleles_frequency_tables_around_sgRNA for given sgRNA  \n",
    "    merge,cols = get_bev_files(bev_list)\n",
    "    \n",
    "    # Calculate log-normalized reads per million for each col\n",
    "    for col in ['#Reads'+col for col in cols]:\n",
    "        colsum = merge[col].sum()\n",
    "        merge.loc[:,str(col+';lognorm')] = merge[col].apply(lambda x: log((float(x)/float(colsum))*1000000 + 1,2))\n",
    "\n",
    "    print(['%Reads'+col for col in cols])\n",
    "    #changed to <2% for poor sequencing quality\n",
    "    # Apply read count filter\n",
    "    merge.loc[:,'%read_count_filter'] = merge.apply(read_count_filter,args=(['%Reads'+col for col in cols],2),axis=1) # less than 1% of all reads\n",
    "    merge.loc[:,'#read_count_filter'] = merge.apply(read_count_filter,args=(['#Reads'+col for col in cols],100),axis=1) # less than 100 reads\n",
    "    \n",
    "    # Before translating, check if there are introns and/or UTRs in translation_ref_seq column in input file\n",
    "    # introns indicated by lowercase letters in translation_ref_seq column \n",
    "    # UTRs indicated by square brackets in translation_ref_seq column \n",
    "    \n",
    "    # Returns true if intron in translation_ref_seq (i.e. if lowercase letters in input translation_ref_seq)\n",
    "    intron_flag = any(c.islower() for c in str(data.loc[data.index[0], 'translation_ref_seq']))\n",
    "    \n",
    "    # Returns true if UTR in translation_ref_seq (i.e. if square brackets in input translation_ref_seq)\n",
    "    utr_flag = any(c=='[' for c in str(data.loc[data.index[0], 'translation_ref_seq']))\n",
    "    \n",
    "#     print(intron_flag)\n",
    "    if utr_flag or intron_flag:\n",
    "        filtered_input_df = pd.DataFrame()\n",
    "        filtered_input_df['old_Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].copy()\n",
    "        filtered_input_df['translation_ref_seq'] = data['translation_ref_seq'].to_list()[0]\n",
    "        filtered_input_df['rev_com'] = data.loc[i, 'rev_com']\n",
    "    \n",
    "    if utr_flag:\n",
    "        print('UTRs exist')\n",
    "#         filtered_input_df = pd.DataFrame()\n",
    "#         filtered_input_df['Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].copy()\n",
    "#         filtered_input_df['translation_ref_seq'] = data['translation_ref_seq'].to_list()[0]\n",
    "#         filtered_input_df['rev_com'] = data.loc[i, 'rev_com']\n",
    "        \n",
    "        # Call remove_utrs function to remove UTRs from aligned sequence before translating \n",
    "        filtered_input_df['Aligned_Sequence']= list(map(remove_utr, filtered_input_df['translation_ref_seq'], filtered_input_df['old_Aligned_Sequence'].copy(), filtered_input_df['rev_com']))\n",
    "#         print(filtered_input_df)\n",
    "        \n",
    "#     else:\n",
    "#         print('no UTRs found')\n",
    "\n",
    "\n",
    "    # if introns exist, remove introns before translating \n",
    "    if intron_flag:\n",
    "        print('introns exist')\n",
    "#         intron_input_df = pd.DataFrame()\n",
    "#         intron_input_df['Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].copy()\n",
    "#         intron_input_df['translation_ref_seq'] = data['translation_ref_seq'].to_list()[0]\n",
    "#         intron_input_df['rev_com'] = data.loc[i, 'rev_com']\n",
    "        if utr_flag: # if already UTR-filtered\n",
    "            print('both introns and UTRs exist')\n",
    "            filtered_input_df['Aligned_Sequence']= list(map(remove_introns, filtered_input_df['translation_ref_seq'], filtered_input_df['Aligned_Sequence'], filtered_input_df['rev_com']))\n",
    "        else:\n",
    "            print('only introns exist')\n",
    "            # Call remove_introns function to remove introns from aligned sequence before translating \n",
    "            filtered_input_df['Aligned_Sequence']= list(map(remove_introns, filtered_input_df['translation_ref_seq'], filtered_input_df['old_Aligned_Sequence'], filtered_input_df['rev_com']))\n",
    "    \n",
    "    # if introns or UTR in translation_ref_seq\n",
    "    if utr_flag or intron_flag: \n",
    "        # Call translate function to translate new (UTR-free and intron-free) sequence  \n",
    "        merge.loc[:,'Translated'] = filtered_input_df['Aligned_Sequence'].apply(translate, args =(row['frame'],row['first_codon'], row['last_codon'], codon_map,))\n",
    "    \n",
    "    # if no introns or UTR\n",
    "    else:\n",
    "        print('no UTRs or introns')\n",
    "        # check if Aligned Sequence needs to be reverse complemented before translating \n",
    "        rev_com = data.loc[i, 'rev_com']\n",
    "        if rev_com:\n",
    "            merge.loc[:,'Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].apply(cfs.revcom)\n",
    "            merge.loc[:,'Reference_Sequence'] = merge.loc[:,'Reference_Sequence'].apply(cfs.revcom)\n",
    "        # Call translate function to translate Aligned Sequence (reverse complemented if necessary) \n",
    "        merge.loc[:,'Translated'] = merge.loc[:,'Aligned_Sequence'].apply(translate,args=(row['frame'],row['first_codon'], row['last_codon'], codon_map,))\n",
    "    \n",
    "#     print(merge['Translated'])\n",
    "    return merge\n",
    "'''\n",
    "Called from: run\n",
    "Function: joins all possible combinations of #Reads;lognorms columns from samples for given sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : #Reads;lognorm columns \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "columns joined by '_' : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column'\n",
    "Ex. '#Reads_BEV_041;lognorm_#Reads_BEV_042;lognorm'  \n",
    "\n",
    "'''\n",
    "\n",
    "def get_corr_name(row):\n",
    "    cols = [row['R1'],row['R2']]\n",
    "    cols.sort()\n",
    "    return '_'.join(cols)\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: gets combinations of samples specified in reps_for_correlation column in correlation input \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "corr_input : correlation input file \n",
    "sg: guide identifier from corr_input\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "combos : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column' for pairs specified in corr_input  \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def get_correlation_cols(corr_input,sg):\n",
    "    corr_input = corr_input.loc[corr_input['sg'] == sg,:]\n",
    "    combos = []\n",
    "    for i,r in corr_input.iterrows():\n",
    "        bevs = r['reps_for_correlation'].split(';')\n",
    "        bevs = ['#Reads_BEV_'+get_bev_str(bev)+';lognorm' for bev in bevs]\n",
    "        if len(bevs) > 1:\n",
    "            combos.extend(itertools.combinations(bevs,2))\n",
    "    combos = ['_'.join(combo) for combo in combos]\n",
    "    return combos\n",
    "\n",
    "hide_toggle(toggle_text_addon='process data functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_9929186813900667063() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_9929186813900667063()\">Show/Hide run function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This function calls all previous functions to generate output tables \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "input_file : input file with columns 'sg', 'translation_ref_seq', 'sgRNA_sequence', 'BEV_start', 'BEV_end', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "corr_input : input file with columns 'sg', 'reps_for_correlation' described above\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "output files stored in output_filepath given above\n",
    "\n",
    "'''\n",
    "\n",
    "def run(input_file,corr_input):\n",
    "    \n",
    "    # List of sgRNA identifiers provided in \"sg\" column of input file\n",
    "    sg_list = list(set(input_file['sg'].tolist())) # drop duplicates from list\n",
    "\n",
    "    # Go through each sgRNA separately    \n",
    "    for sg in sg_list:\n",
    "        print(sg)\n",
    "        \n",
    "        # Filter input file to contain only rows for given sgRNA\n",
    "        data = input_file.loc[input_file['sg'] == sg,:]\n",
    "        \n",
    "        # Merge all the allele read counts\n",
    "        # To do this, drop the BEV_test and BEV_ref columns (which just have information needed for the LFC calculation)\n",
    "        # Then drop duplicate rows\n",
    "        data_dedup = data.drop_duplicates(subset=['sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'])\n",
    "        \n",
    "        # Call process_data_v2 function which adds the following columns to output table:\n",
    "        #read counts, lognorms, aligned sequences, reference sequences, translations \n",
    "        merge = process_data_v2(data_dedup)\n",
    "        \n",
    "        # Get the WT column\n",
    "        merge['WT'] = merge.apply(get_wt_col,axis=1)\n",
    "        \n",
    "#         print(merge[merge['WT']])\n",
    "        # Now, go through each row and calculate the LFC for each of the pairs specified in BEV_test and BEV_ref\n",
    "        for i,r in data.iterrows():               \n",
    "            merge = get_lfc_v2(r,merge)\n",
    "        print('num rows in merge df:', len(merge))\n",
    "        print(merge[merge['%read_count_filter'] == True])\n",
    "            \n",
    "        # Write out 2 files: full file (merge) and filtered file (only including alleles with > 1% reads in at least one condition)\n",
    "        Path(output_filepath).mkdir(parents=True, exist_ok=True)\n",
    "        merge.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_allele_frequency_table_around_sgRNA.csv',index=False)\n",
    "        filtered = merge[merge['%read_count_filter'] == True]\n",
    "        filtered.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_filtered_allele_frequency_table_around_sgRNA.csv',\n",
    "                        index=False)            \n",
    "        \n",
    "        filtered_adjusted_percentage = filtered.copy()\n",
    "        \n",
    "        # Adjust percentages in filtered table so they total 100% \n",
    "        \n",
    "        # Get #Reads columns from filtered table \n",
    "        num_reads_cols_all = [col for col in filtered_adjusted_percentage.columns if '#Reads' in col]\n",
    "        # List lognorm column names that are included in #Reads cols \n",
    "        lognorm_cols = [col for col in num_reads_cols_all if 'lognorm' in col]\n",
    "        # Drop lognorm columns \n",
    "        num_reads_cols = [col for col in num_reads_cols_all if col not in lognorm_cols]\n",
    "        \n",
    "#         print(num_reads_cols)\n",
    "        \n",
    "        for col in num_reads_cols:\n",
    "            num_reads_col = col\n",
    "            sum_reads = filtered_adjusted_percentage[num_reads_col].sum()\n",
    "            print(sum_reads)\n",
    "#             percentage_reads_col ='%Reads_BEV_'+ get_bev_str(num)\n",
    "            percentage_reads_col = num_reads_col.replace('#', '%')\n",
    "            for i in filtered_adjusted_percentage.index:\n",
    "                num_reads = filtered_adjusted_percentage.loc[i, num_reads_col]\n",
    "                adjusted_percentage_reads = (num_reads/sum_reads) * 100 \n",
    "                filtered_adjusted_percentage.at[i, percentage_reads_col] = adjusted_percentage_reads\n",
    "                \n",
    "#         print(filtered_adjusted_percentage)\n",
    "        \n",
    "        filtered_adjusted_percentage.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_filtered_adj_percentage_allele_frequency_table_around_sgRNA.csv',\n",
    "                index=False) \n",
    "\n",
    "        # Get correlations matrix of log-normalized rpm, using only alleles with > 100 reads in at least one sample\n",
    "        merge = merge[merge['#read_count_filter'] == True]\n",
    "        cols = [x for x in list(merge) if 'lognorm' in x]\n",
    "        correlations = merge[cols].corr(method='pearson')\n",
    "        correlations['R1'] = correlations.index\n",
    "        correlations = correlations.melt(id_vars = 'R1', value_vars=list(correlations).remove('R1'),var_name='R2',value_name='Pearson')\n",
    "        \n",
    "        # Drop correlations that are not specified in corr_input\n",
    "        correlations['Reps'] = correlations.apply(get_corr_name,axis=1)\n",
    "        combos = get_correlation_cols(corr_input,sg)\n",
    "        correlations = correlations[correlations['Reps'].isin(combos)]\n",
    "        \n",
    "        # Drop duplicate rows (i.e. A vs B and B vs A)\n",
    "        correlations = correlations.drop_duplicates(subset=['Reps'])\n",
    "        \n",
    "        # Write to file\n",
    "        Path(output_filepath + \"corr_outputs/#read_count_filter_pearson/\").mkdir(parents=True, exist_ok=True)\n",
    "        correlations.to_csv(output_filepath + \"corr_outputs/#read_count_filter_pearson/sg\" +str(r['sg'])+'_'+r['primer']+'_correlations.csv')\n",
    "#         break\n",
    "    return \n",
    "\n",
    "hide_toggle(toggle_text_addon='run function')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_14978550320641788091() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_14978550320641788091()\">Show/Hide heatmap function</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "rcParams['font.family'] = 'monospace' #so translated sequences are aligned\n",
    "rcParams['font.monospace'] = 'Arial'\n",
    "rcParams['font.size']: 12.0\n",
    "\n",
    "'''\n",
    "This function generates allele-level heatmaps using log-fold changes for each validation condtion\n",
    "annotated with the corresponding allele amino acid sequences\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "df : input file with columns 'sg', 'translation_ref_seq', 'sgRNA_sequence', 'BEV_start', 'BEV_end', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "vmin : sets minimum value for heatmap color bar\n",
    "vmax: sets maximum value for heatmap color bar\n",
    "filepath: same as output_filepath specified above where allele frequency output tables are saved \n",
    "time1: (optional) string describing early time point for table header, e.g. 'Day7' (default: 'Time1')\n",
    "time2: (optional) string describing late time point for table header, e.g. 'Day14' (default: 'Time2')\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "1. allele-level heatmpas with corresponding allele amino acid sequences annotated\n",
    "2. tables with average % reads for each allele at early and late time points \n",
    "\n",
    "'''\n",
    "\n",
    "def heatmaps(df,vmin,vmax,filepath, time1 = 'Time1', time2 = 'Time2', timepoint_df=None, filename = 'translation_heatmap', **kwargs):\n",
    "\n",
    "    num_rows = len(df)\n",
    "    num_cols = 2\n",
    "    fig,axs = plt.subplots(num_rows, num_cols, figsize=(num_cols*4,num_rows*4))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace = 0.5)\n",
    "    \n",
    "    row_count = 0\n",
    "    \n",
    "    for num, (i,r) in enumerate(df.iterrows()):\n",
    "\n",
    "        sg = r['sg']\n",
    "#         print('before plot sg: ', sg)\n",
    "        primer = r['primer']\n",
    "#         location = filepath+str(sg)+'_'+primer+'_filtered_allele_frequency_table_around_sgRNA.csv'\n",
    "        location = filepath+str(sg)+'_'+primer+'_filtered_adj_percentage_allele_frequency_table_around_sgRNA.csv'\n",
    "\n",
    "        to_plot = pd.read_csv(location)\n",
    "\n",
    "        avgLFC_col = [col for col in to_plot.columns if 'AvgLFC' in col][0]\n",
    "\n",
    "        to_plot.sort_values(by=avgLFC_col,ascending=False,inplace=True)\n",
    "        \n",
    "        # Earlier time point -> Time1\n",
    "        time1_avg_col = 'Avg%Reads_' + time1\n",
    "        \n",
    "        # Later time point -> Time 2\n",
    "        time2_avg_col = 'Avg%Reads_' + time2\n",
    "        \n",
    "        if timepoint_df is None: # Default to BEV_ref as Time1 samples and BEV_test as Time2 samples\n",
    "            \n",
    "            # Default Time 1 (Earlier time point) samples  = BEV_ref columns \n",
    "            time1_samples = r['BEV_ref'].split(';') # convert to list of BEV numbers\n",
    "\n",
    "            # Default Time 2 (Later time point) samples  = BEV_ref columns \n",
    "            time2_samples = r['BEV_test'].split(';') # convert to list of BEV numbers \n",
    "            \n",
    "        else: # if timepoint_df provided\n",
    "            # Get Time 1 columns from df based on time1 parameter \n",
    "            sg_timepoint_df = timepoint_df[timepoint_df['sg']==sg]\n",
    "            time1_sample_df = sg_timepoint_df[sg_timepoint_df['time_point']==time1].reset_index(drop=True)\n",
    "            time1_samples = time1_sample_df.loc[0, 'BEV_nums'].split(';')\n",
    "            \n",
    "            # Get Time 2 columns from df based on time2 parameter \n",
    "            sg_timepoint_df = timepoint_df[timepoint_df['sg']==sg]\n",
    "            time2_sample_df = sg_timepoint_df[sg_timepoint_df['time_point']==time2].reset_index(drop=True)\n",
    "            time2_samples = time2_sample_df.loc[0, 'BEV_nums'].split(';')\n",
    "        \n",
    "        # Lists to store correctly formatted %Reads column names \n",
    "        time1_cols = [] \n",
    "        time2_cols = []\n",
    "        \n",
    "        # %Reads columns named \"%Reads_BEV_\" + get_bev_str(int(BEV_num))\n",
    "        for num in time1_samples:\n",
    "            col_name = \"%Reads_BEV_\" + get_bev_str(int(num))\n",
    "            time1_cols.append(col_name)  \n",
    "\n",
    "        to_plot[time1_avg_col] = round(to_plot[time1_cols].apply(np.mean,axis=1),1).apply(str)#+'%'\n",
    "\n",
    "        # %Reads columns named \"%Reads_BEV_\" + get_bev_str(int(BEV_num))\n",
    "        for num in time2_samples:\n",
    "            col_name = \"%Reads_BEV_\" + get_bev_str(int(num))\n",
    "            time2_cols.append(col_name)  \n",
    "\n",
    "        to_plot[time2_avg_col] = round(to_plot[time2_cols].apply(np.mean,axis=1),1).apply(str)#+'%'\n",
    "\n",
    "        to_plot['Label'] = to_plot['Translated']\n",
    "\n",
    "        reads_df = to_plot[[time1_avg_col, time2_avg_col]]\n",
    "\n",
    "        to_plot.loc[to_plot['WT'],'Label'] = to_plot.loc[to_plot['WT'],'Label'].values[0] + '<wt' \n",
    "\n",
    "        heatmap_df = to_plot[['Label',[col for col in to_plot.columns if 'AvgLFC' in col][0]]].reset_index(drop = True)\n",
    "        \n",
    "        if row_count < num_rows:\n",
    "            # HEATMAP\n",
    "            if num_rows == 1: \n",
    "                heatmap_ax = axs[0]\n",
    "            else: \n",
    "                heatmap_ax = axs[row_count, 0]\n",
    "            \n",
    "            cmap = LinearSegmentedColormap.from_list(name='test', colors=['#8DA0CB','white','#F8774F'])\n",
    "            sns.heatmap(heatmap_df.set_index('Label'),cmap=cmap,vmin=vmin,vmax=vmax,ax=heatmap_ax,annot=True, yticklabels = True, fmt='.1f')\n",
    "            label_list = [item for item in heatmap_ax.get_yticklabels()]\n",
    "            for label in label_list:\n",
    "                if '<wt' in label.get_text():\n",
    "                    label.set_color('red')\n",
    "            plt.setp(heatmap_ax.get_yticklabels(), rotation=0, ha=\"left\", size = 12)#,rotation_mode=\"anchor\")\n",
    "\n",
    "            yax = heatmap_ax.get_yaxis()\n",
    "            # find the maximum width of the label on the major ticks\n",
    "            pad = max(T.label.get_window_extent().width for T in yax.majorTicks)\n",
    "            yax.set_tick_params(pad=pad)\n",
    "            heatmap_ax.set_ylabel('', rotation = 0)\n",
    "            heatmap_ax.set_title(sg)\n",
    "                    \n",
    "            bbox = heatmap_ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            width, height = bbox.width, bbox.height\n",
    "\n",
    "            # TABLE\n",
    "            cell_text = []\n",
    "            for row in range(len(reads_df)):\n",
    "                cell_text.append(reads_df.iloc[row])\n",
    "            \n",
    "            if num_rows == 1: \n",
    "                table_ax = axs[1]\n",
    "            else: \n",
    "                table_ax = axs[row_count, 1]\n",
    "            #print('table position:', table_ax)\n",
    "            translations = list(heatmap_df['Label'])\n",
    "            #print(translations)\n",
    "            cell_height = height/(2*(len(translations)))\n",
    "\n",
    "            reads_table = table_ax.table(cellText=cell_text, colLabels=reads_df.columns, loc='center', cellLoc = 'center', edges = 'open')#, bbox = [0,0, width, height/2.3])\n",
    "            reads_table.auto_set_font_size(False)\n",
    "            reads_table.set_fontsize(12)\n",
    "            reads_table.auto_set_column_width(col=list(range(len(reads_df.columns))))\n",
    "            #reads_table.scale(1, 1.5)\n",
    "            cellDict = reads_table.get_celld()\n",
    "            \n",
    "            for i in range(0,len(reads_df.columns)):\n",
    "                cellDict[(0,i)].set_height(cell_height)\n",
    "                for j in range(1,len(reads_df)+1):\n",
    "                    cellDict[(j,i)].set_height(cell_height)\n",
    "            table_ax.set_axis_off()\n",
    "            \n",
    "\n",
    "            row_count+=1\n",
    "#             print(row_count)\n",
    "#             break\n",
    "    # Create path to Figures folder if doesn't exist already\n",
    "    Path(filepath + '/Figures/').mkdir(parents=True, exist_ok=True)\n",
    "    full_filepath = filepath + 'Figures/' + filename +'.pdf'\n",
    "    print(full_filepath)\n",
    "    plt.savefig(full_filepath,bbox_inches=\"tight\")\n",
    "\n",
    "hide_toggle(toggle_text_addon='heatmap function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "<font color='blue'> Please follow steps indicated in blue, then run the notebook to generate output files. If the files are formatted as described in the documentation, the code in the 'Functions' section should not need to be altered. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metainformation file** \n",
    "\n",
    "<font color='blue'> <b>Step 1:</b> Create metainformation input file in the following format. </font>\n",
    "\n",
    "For sequence inputs please refer to the [Sequence_Orientation_Documentation](../docs/Sequence_Orientation_Documentation.html).\n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **sgRNA_sequence** : sequence of sgRNA as designed \n",
    "* **translation_ref_seq**: reference sequence outputted by CRISPResso formatted such that any intronic sequences are lower-case, exons are upper-case, and UTRs are indicated by square brackets (if applicable) <u> must be sequence on strand that is being translated; may not necessarily be the same strand as the sgRNA sequence</u> \n",
    "    * Ex. <font color='grey'>tgtcttttctatgatctctttag</font><font color='green'>GGGTGACCCAGTCTATT</font>\n",
    "* **BEV_start** : BEV number for first sample in sg\n",
    "* **BEV_end** : BEV number for last sample in sg\n",
    "* **primer** : name of primer pair (joined by '\\_') used to amplify genomic locus as mentioned in sample name\n",
    "    * Ex. <font color='purple'>F_C12</font><font color = 'blue'><b>_</b></font><font color='green'>R_C12</font>\n",
    "* **frame** : frame for translation (manually determined for each sg / primer pair); position of first coding nucleotide in reference sequence within codon; frame can be 1, 2, 3\n",
    "    * Ex. given reference sequence: tgtcttttctatgatctctttag<font color='green'>**G**</font>G|GTG|ACC|CAG|TCT|ATT \n",
    "        since the first coding nucleotide of the reference sequence (<font color='green'><b>G</b></font>) is the 2nd nucleotide in its codon \n",
    "        (\\_<font color='green'><b>G</b></font>G) &rightarrow; frame = 2\n",
    "* **first_codon** : first codon for translation \n",
    "* **last_codon** : last codon for translation \n",
    "* **rev_com** : samples for which reference sequence is on reverse strand \n",
    "* **BEV_ref** : reference sample(s) for log-fold change (LFC) calculation (i.e. early time point, empty vector, etc.); if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "* **BEV_test** : test sample(s) for LFC calculation; if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "\n",
    "**Example input:**\n",
    "\n",
    "\n",
    "| sg      | sgRNA_sequence       | translation_ref_seq                                  | BEV_start | BEV_end | primer        | frame | first_codon| last_codon| rev_com | BEV_ref | BEV_test |\n",
    "| ------- | -------------------- | ---------------------------------------- |  -------: |  -----: | ------------- |  ----|----|---: | ------: | ------- | -------- |\n",
    "| 397   | GTCACCCCTAAAGAGATCAT | tgtcttttctatgatctctttagGGGTGACCCAGTCTATT | 7         | 12      |F_C12_R_C12 |  2    |TGG|ATT| True    | 5;6     | 9;10     |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> <b> Step 2: </b> Enter filepath to metainformation input file here </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_file = pd.read_csv('MOLM13_BCL2_allvalidation/Metainfo_input_BCL2_CBE_first_last_codon.csv')\n",
    "input_filepath = input(\"Please enter input filepath here: \")\n",
    "input_file = pd.read_csv(input_filepath)\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for NaN values i.e. blank rows\n",
    "if input_file.isnull().values.any(): \n",
    "    input_file = clean_input_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_input_file(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Metainformation file** \n",
    "\n",
    "<font color='blue'><b>Step 3:</b> Create correlation input file in the following format  </font> \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **reps_for_correlation** : semicolon-separated BEV numbers of which to calculate the pairwise Pearson correlation of the log-normalized read counts\n",
    "\n",
    "**Example input:**\n",
    "    \n",
    "| sg      | reps_for_correlation |\n",
    "| ------- | -------------------: | \n",
    "| 397     | 7;8 | \n",
    "| 397     | 9;10 | \n",
    "| 397     | 11;12 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 4:</b> Enter filepath to correlation input file here  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corr_input = pd.read_csv('MOLM13_BCL2_allvalidation/Correlation_input_CBE.csv')\n",
    "corr_corr_inputpath = input(\"Please enter correlation input filepath here: \")\n",
    "corr_input = pd.read_csv(corr_corr_inputpath)\n",
    "corr_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify folder filepaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 5:</b> Enter filepath to folder containing CRISPResso output files here. Please make sure that the filepath does not begin with a '/' but does end in a '/'.  </font> \n",
    "\n",
    "Please note that each folder containing CRISPResso output files for individual samples within the given folder should be named in the format 'CRISPResso_on_'+bev+'\\_'+\n",
    "primer, where bev = ('BEV' or 'NGBEV') + sample_number and primer = primer name. \n",
    "Ex. <font color='grey'>CRISPResso_on</font><font color='purple'>_BEV_001</font><font color='green'>_F2_R2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global bev_string_id\n",
    "bev_string_id = input('Please enter either \\'BEV\\' or \\'NGBEV\\' to indicate which string is used when naming your CRISPResso files.')\n",
    "if ((bev_string_id != 'BEV') and (bev_string_id != 'NGBEV')):\n",
    "    raise Exception('Invalid input. Please enter either \\'BEV\\' or \\'NGBEV\\' to specify which string is used in CRISPResso file names. Be careful not to add any extra spaces.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global CRISPResso_filepath \n",
    "# CRISPResso_filepath = 'MOLM13_BCL2_allvalidation/CRISPRessoBatch_on_FR_batch_file/'\n",
    "CRISPResso_filepath = input(\"Please enter CRISPResso filepath here: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 6:</b> Enter filepath to folder where the files generated by this notebook will be stored. Please make sure that the filepath does not begin with a '/' but does end in a '/'. If the folders in this file path do not currently exist, they will be created when the notebook is run.  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to store allele_freq output tables \n",
    "global output_filepath \n",
    "# output_filepath = 'MOLM13_BCL2_allvalidation/Results/CBE_v2/2percent_filter/'\n",
    "output_filepath = input(\"Please enter output folder filepath here: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the two input files and produce allele tables for all sgRNAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(input_file,corr_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate allele-level heat maps annotated with corresponding allele amino acid sequences and tables with average % reads for each allele at early and late time points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, early time point samples are considered to be the same as the BEV_ref samples and the late time point samples are considered to be the BEV_test samples. If this is not the case (e.g.,  BEV_test samples span multiple time points), please upload a file with the following columns: \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier \n",
    "* **Time point** : string that identifies time point \n",
    "* **BEV_num** : semicolon-separated BEV numbers corresponding to samples at that time point \n",
    "\n",
    "**Example input:**\n",
    "    \n",
    "| sg      |time_point | BEV_nums |\n",
    "| ------- |------- | -------------------: | \n",
    "| 397     |D8      |  7;8 | \n",
    "| 397     |D14      | 9;10 | \n",
    "| 397     |D21      | 11;12 | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global timepoint_input_file\n",
    "\n",
    "# Check if default settings work for user\n",
    "default_y_or_n = input(\"Do the default time point sample assignments work for you? Please enter 'y' or 'n'. If 'n', you will be asked to enter the path to a time point input file as described above. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_y_or_n = default_y_or_n.lower()\n",
    "if default_y_or_n == 'y':\n",
    "    timepoint_input_file = None\n",
    "#     print('y')\n",
    "elif default_y_or_n == 'n':\n",
    "    timepoint_input_filepath = input(\"Please enter input filepath here: \")\n",
    "    # timepoint_input_file = pd.read_csv('AnnabelData/timepoint_df_test.csv')\n",
    "    timepoint_input_file = pd.read_csv(timepoint_input_filepath)\n",
    "#     print('n')\n",
    "else:\n",
    "    raise Exception('Invalid input. Please enter either \\'y\\' or \\'n\\' and re-run the cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps(df = input_file,\n",
    "         vmin = -2,\n",
    "         vmax = 2,\n",
    "         filepath = output_filepath,\n",
    "         time1 = 'D7',\n",
    "         time2 = 'D21',\n",
    "         timepoint_df = timepoint_input_file,\n",
    "         filename = 'BCL2_CBE_heatmap'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}