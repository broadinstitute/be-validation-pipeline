{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making allele frequency plots\n",
    "\n",
    "This notebook contains code to process the CRISPResso \"allele frequency table\" files from base editor validation experiments. The inputs are 2 input files: the first contains metainformation about each sample to make the \"allele frequency\" file, and the second contains metainformation to compute correlations between the log-normalized read counts. The output of this file are 3 files for each sgRNA / primer pair: \n",
    "1. a file containing all alleles and their read counts for each sample\n",
    "2. a filtered version of (1) that only contains alleles with at least 1% abundance in any sample\n",
    "3. a file containing the Pearson correlations between log-normalized read counts of each allele with > 100 reads in at least one sample\n",
    "\n",
    "(1) is the starting file used to show the abundance of specific edits over time (code in BEV_aa_over_time.ipynb). (2) is the starting file used to create allele-level heatmaps (this was done using GraphPad Prism). (3) is the starting file for the plots showing the replicate correlation for validation experiments (actual plots were made using GraphPad Prism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import base_edit_functions as be\n",
    "from math import log\n",
    "from os import path\n",
    "import sys, itertools\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version: ' + sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modules = ['pandas', 'numpy']\n",
    "for module in modules:\n",
    "    try:\n",
    "        print(module + ' ' + sys.modules[module].__version__)\n",
    "    except:\n",
    "        print(module + ' has no __version__ attribute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "<font color='blue'> Please follow steps indicated in blue, then run the notebook to generate output files. If the files are formatted as described in the documentation, the code in the 'Functions' section should not need to be altered. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metainformation file** \n",
    "\n",
    "<font color='blue'> <b>Step 1:</b> Create metainformation input file in the following format </font> \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier written in the following format: <font color='green'>\\[cell line\\]</font> <font color='grey'>_RDA</font><font color='purple'>\\[RDA number\\]</font> \n",
    "    * Ex. <font color='green'>HAP1</font><font color = 'grey'>\\_RDA</font><font color = 'purple'>397 </font>\n",
    "* **sgRNA_sequence** : sequence of sg\n",
    "* **ref_seq**: reference sequence outputted by CRISPResso formatted such that intronic sequences are lower-case and exons are upper-case (if applicable)\n",
    "    * Ex. <font color='grey'>tgtcttttctatgatctctttag</font><font color='green'>GGGTGACCCAGTCTATT</font>\n",
    "* **BEV_start** : BEV number for first sample in sg\n",
    "* **BEV_end** : BEV number for last sample in sg\n",
    "* **primer** : name of primer pair (joined by '\\_') used to amplify genomic locus as mentioned in sample name\n",
    "    * Ex. <font color='purple'>F_C12</font><font color = 'blue'><b>_</b></font><font color='green'>R_C12</font>\n",
    "* **frame** : frame for translation (manually determined for each sfg / primer pair); position of first coding nucleotide in reference sequence within codon; frame can be 1, 2, 3\n",
    "    * Ex. given reference sequence: tgtcttttctatgatctctttag<font color='green'>**G**</font>G|GTG|ACC|CAG|TCT|ATT \n",
    "        since the first coding nucleotide of the reference sequence (<font color='green'><b>G</b></font>) is the 2nd nucleotide in its codon \n",
    "        (\\_<font color='green'><b>G</b></font>G) &rightarrow; frame = 2\n",
    "* **rev_com** : samples for which reverse complement parameter was used to run CRISPResso \n",
    "* **BEV_ref** : reference sample(s) for log-fold change (LFC) calculation; if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "* **BEV_test** : test sample(s) for LFC calculation; if multiple BEV numbers are given, they should be separated by ';', and they will be treated as replicates that will be averaged\n",
    "\n",
    "**Example input:**\n",
    "\n",
    "\n",
    "| sg      | sgRNA_sequence       | ref_seq                                  | BEV_start | BEV_end | primer        | frame | rev_com | BEV_ref | BEV_test |\n",
    "| ------- | -------------------- | ---------------------------------------- |  -------: |  -----: | ------------- |  ---: | ------: | ------- | -------- |\n",
    "| HAP1_RDA397   | GTCACCCCTAAAGAGATCAT | tgtcttttctatgatctctttagGGGTGACCCAGTCTATT | 7         | 12      |F_C12_R_C12 |  2    | True    | 5;6     | 9;10     |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> <b> Step 2: </b> Enter filepath to metainformation input file here </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input_file = pd.read_csv('ZsofiaData/BCV_allele_freq_input_for_PR_fixed_frames.csv')\n",
    "input_filepath = input(\"Please enter input filepath here: \")\n",
    "input_file = pd.read_csv(input_filepath)\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Metainformation file** \n",
    "\n",
    "<font color='blue'><b>Step 3:</b> Create correlation input file in the following format  </font> \n",
    "\n",
    "**Columns**: \n",
    "\n",
    "* **sg** : sg identifier written in the following format: <font color='green'>\\[cell line\\]</font> <font color='grey'>_RDA</font><font color='purple'>\\[RDA number\\]</font> \n",
    "    * Ex. <font color='green'>HAP1</font><font color = 'grey'>\\_RDA</font><font color = 'purple'>397 </font>\n",
    "* **reps_for_correlation** : semicolon-separated BEV numbers of which to calculate the pairwise Pearson correlation of the log-normalized read counts\n",
    "\n",
    "**Example input:**\n",
    "    \n",
    "| sg      | reps_for_correlation |\n",
    "| ------- | -------------------: | \n",
    "| 397     | 7;8 | \n",
    "| 397     | 9;10 | \n",
    "| 397     | 11;12 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 4:</b> Enter filepath to correlation input file here  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#corr_input = pd.read_csv('ZsofiaData/BCV_allele_corr_input.csv')\n",
    "corr_input_filepath = input(\"Please enter correlation input filepath here: \")\n",
    "corr_input = pd.read_csv(corr_input_filepath)\n",
    "corr_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify folder filepaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 5:</b> Enter filepath to folder containing CRISPResso output files here  </font> \n",
    "\n",
    "Please note that each folder containing CRISPResso output files for individual samples within the given folder should be named in the format 'CRISPResso_on_'+bev+'\\_'+\n",
    "primer, where bev = 'BEV' + sample_number and primer = primer name. \n",
    "Ex. <font color='grey'>CRISPResso_on</font><font color='purple'>_BEV_001</font><font color='green'>_F2_R2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global CRISPResso_filepath \n",
    "#CRISPResso_filepath = 'ZsofiaData/CRISPRessoBatch_on_Crispresso_BE_batch_file_input/'\n",
    "CRISPResso_filepath = input(\"Please enter CRISPResso filepath here: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'><b>Step 6:</b> Enter filepath to folder where the files generated by this notebook will be stored. If the folders in this file path do not currently exist, they will be created when the notebook is run.  </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to store allele_freq output tables \n",
    "global output_filepath \n",
    "#output_filepath = 'ZsofiaData/Validation_CRISPResso_results/allele_freq/'\n",
    "output_filepath = input(\"Please enter output folder filepath here: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> <b>Ready to run the notebook!</b> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function removes any NaN rows from input_file\n",
    "'''\n",
    "def clean_input_file(df):\n",
    "    df = df.dropna() #drop NaN \n",
    "    #dropna() converts int to float, so convert them back\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns #select subset of df of type float\n",
    "    for col in float_cols: \n",
    "        #get original column index so can replace at correct loc\n",
    "        index = df.columns.get_loc(col)\n",
    "        #rename float cols as \"float_\"col name \n",
    "        float_col_name = 'float_' + col\n",
    "        df = df.rename(columns = {col : float_col_name})\n",
    "        #overwrite as type int\n",
    "        float_to_int = df[float_col_name].astype(int).copy()\n",
    "        df.insert(index, col, float_to_int)\n",
    "        #drop float column \n",
    "        df = df.drop(float_col_name, axis = 1)\n",
    "    return df\n",
    "\n",
    "#check for NaN values i.e. blank rows\n",
    "if input_file.isnull().values.any(): \n",
    "    input_file = clean_input_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: converts the BEV number from an int to a 3-digit string\n",
    "'''\n",
    "def get_bev_str(bev):\n",
    "    bev = int(bev)\n",
    "    if bev < 10:\n",
    "        return '00'+str(bev)\n",
    "    if bev < 100:\n",
    "        return '0'+str(bev)\n",
    "    return str(bev)\n",
    "'''\n",
    "Called from: get_path\n",
    "Function: Verifies existence of filepath generated by get_path function to retrieve 'Alleles_frequency_table_around_sgRNA' file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "def check_filepath(filepath,bev,primer,sg_seq):\n",
    "    file_loc = filepath+'/CRISPResso_on_'+bev+'_'+primer+'/'+'Alleles_frequency_table_around_sgRNA_'+sg_seq+'.txt'\n",
    "    if path.exists(file_loc):\n",
    "        return file_loc\n",
    "    return ''\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: calls check_filepath to get filepath where CRISPResso output files are stored, \n",
    "whose folder path (CRISPResso_filepath) is provided above\n",
    "Calls: check_filepath\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_num : BEV sample number\n",
    "primer: primer name (from input file)\n",
    "sg_seq: guide sequence\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "file_loc : filepath of 'Alleles_frequency_table_around_sgRNA_[].txt' file if exists\n",
    "None if no filepath exists \n",
    "\n",
    "'''\n",
    "\n",
    "def get_path(bev_num,primer,sg_seq):\n",
    "    bev = 'BEV_' + get_bev_str(bev_num)\n",
    "    filepath = CRISPResso_filepath\n",
    "    return check_filepath(filepath,bev,primer,sg_seq)\n",
    "\n",
    "\n",
    "'''\n",
    "Called from: get_bev_files\n",
    "Function: merges together the \"Allele_frequency_table_around_sgRNA\" files\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "filepath : filepath to 'Alleles_frequency_table_around_sgRNA_[].txt' file in CRISPResso output folder, \n",
    "           from get_path function\n",
    "bev : BEV sample number \n",
    "sg_seq: guide sequence\n",
    "existing_df: merge data frame with 'Aligned_Sequence' and 'Reference_Sequence' columns defined in get_bev_files\n",
    "cols: empty list populated in function \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with Aligned_Sequence, Reference_Sequence, Reads columns from each sample \n",
    "cols: columns labeled with BEV sample number \n",
    "'''\n",
    "def merge_bev_file(filepath,bev,sg_seq,existing_df,cols):\n",
    "    if not path.exists(filepath):\n",
    "        print('no file')\n",
    "        return existing_df,cols\n",
    "    df = pd.read_table(filepath,index_col=False)\n",
    "    # Sum together any rows that share both 'Aligned Sequence' and 'Reference Sequence' with each other (this is rare)\n",
    "    df_summed = df[['Aligned_Sequence','Reference_Sequence','#Reads','%Reads']].groupby(['Aligned_Sequence','Reference_Sequence'],as_index=False).agg('sum')\n",
    "    cols.append(str('_BEV_'+str(bev)))\n",
    "    df_summed = df_summed.rename(columns={'#Reads':str('#Reads_BEV_'+str(bev)),'%Reads':str('%Reads_BEV_'+str(bev))})\n",
    "    # Outer merge onto existing dataframe\n",
    "    merge = pd.merge(existing_df,df_summed,how='outer',on=['Aligned_Sequence','Reference_Sequence'])\n",
    "    # Fill in nans with 0\n",
    "    merge = merge.fillna(0)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: function gets and merges all \"Allele_frequency_table_around_sgRNA\" files for a given sgRNA sequence\n",
    "(i.e. different replicates, drug conditions, etc.)\n",
    "This is customized to work with files with the \"BEV\" notation\n",
    "Calls: get_path, merge_bev_file\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "bev_list : contains BEV sample number, primer name, guide sequence for each sample  \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge: merged dataframe with \n",
    "cols:\n",
    "\n",
    "'''\n",
    "\n",
    "def get_bev_files(bev_list):\n",
    "    merge = pd.DataFrame(columns=['Aligned_Sequence','Reference_Sequence'])\n",
    "    cols = []\n",
    "    for bev,sg_seq,primer_name in bev_list:\n",
    "        filepath = get_path(bev,primer_name,sg_seq)\n",
    "        if filepath != '':\n",
    "            merge,cols = merge_bev_file(filepath=filepath,bev=bev,sg_seq=sg_seq,existing_df=merge,cols=cols)\n",
    "    return merge,cols\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: filters out rows that don't meet given threshold (<1% for %Reads and <100 for #Reads)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row in column to which function is being applied (%Reads or #Reads)\n",
    "cols: given cols (%Reads, #Reads for all samples )\n",
    "val: threshold for filter \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns False for any rows (alleles) that have a value < the given value in ALL of the given cols\n",
    "\n",
    "'''\n",
    "def read_count_filter(row,cols,val):\n",
    "    for col in cols:\n",
    "        if row[col] > val:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: removes introns from Aligned_Sequence before translation\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "ref_seq: reference sequence from input file with intron sequence indicated by lowercase \n",
    "aligned: Aligned_sequence from merge df \n",
    "rev_com: True (from input file) if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "new_aligned: aligned sequence without introns and reverse complemented if necessary \n",
    "\n",
    "'''\n",
    "def remove_introns(ref_seq, aligned, rev_com):\n",
    "    #check for introns\n",
    "    introns = re.compile('[a-z]+').findall(ref_seq)\n",
    "    if not introns:\n",
    "        #print('no introns')\n",
    "        return \n",
    "    new_aligned = aligned.upper()\n",
    "    new_introns = []    \n",
    "    if rev_com:\n",
    "        for i in introns:\n",
    "            rev_com_intron = be.revcom(i.upper())\n",
    "            new_introns.append(rev_com_intron)\n",
    "    else:\n",
    "        new_introns = introns\n",
    "    for i in new_introns:\n",
    "        if new_aligned.find(i.upper()) == -1: # if wild type intron not found, a splice site mutation may have occurred\n",
    "            #print('not found')\n",
    "            new_aligned = False\n",
    "            #return new_aligned\n",
    "        else: # if wild type intron found\n",
    "            #print('found!')\n",
    "            new_aligned = new_aligned.replace(i.upper(), '')\n",
    "            if rev_com:\n",
    "                new_aligned = be.revcom(new_aligned)\n",
    "        return new_aligned\n",
    "\n",
    "    \n",
    "'''\n",
    "Called from: process_data_v2\n",
    "Function: translates Aligned_sequence to give amino acid sequence\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "seq: Aligned_sequence (after removing introns and/or reverse complementing if applicable)\n",
    "frame: frame from input file \n",
    "rev_com: from input file; True if sequence needs to be reverse complemented \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "aa: amino acid sequence\n",
    "\n",
    "'''\n",
    "# This function returns the translation of a given sequence and frame\n",
    "# This function returns the translation of a given sequence and frame\n",
    "def translate(seq, frame, codon_map):\n",
    "    if not seq: # if remove_introns returned False -> possible splice site mutation\n",
    "        return 'Possible splice site mutation'\n",
    "    aa = ''\n",
    "    i = 0\n",
    "    while i < len(seq)-1:\n",
    "        substring = ''\n",
    "        while frame <= 3:\n",
    "            if i<len(seq):\n",
    "                if seq[i] == '-':\n",
    "                    #print('deletion')\n",
    "                    i += 1\n",
    "                    # frame doesn't change\n",
    "                else:\n",
    "                    substring += seq[i]  \n",
    "                    i += 1\n",
    "                    frame+=1\n",
    "            else: # if reached end of the sequence and frame still <=3\n",
    "                frame = 4\n",
    "                continue\n",
    "            #print('substring:', substring, 'i = ', i, 'frame = ', frame)\n",
    "        if len(substring) == 3:\n",
    "            frame = 1\n",
    "            #if 'N' in substring:\n",
    "            if ('N' in substring):\n",
    "                aa = aa + '-'\n",
    "\n",
    "            else:\n",
    "                aa = aa + codon_map[substring]\n",
    "        else:\n",
    "            frame = 1\n",
    "    return aa\n",
    "\n",
    "codon_map = {'TTT':'F', 'TTC':'F', 'TTA':'L', 'TTG':'L', 'CTT':'L', 'CTC':'L', 'CTA':'L', 'CTG':'L', 'ATT':'I', 'ATC':'I',\n",
    "             'ATA':'I', 'ATG':'M', 'GTT':'V', 'GTC':'V', 'GTA':'V', 'GTG':'V', 'TCT':'S', 'TCC':'S', 'TCA':'S', 'TCG':'S',\n",
    "             'CCT':'P', 'CCC':'P', 'CCA':'P', 'CCG':'P', 'ACT':'T', 'ACC':'T', 'ACA':'T', 'ACG':'T', 'GCT':'A', 'GCC':'A',\n",
    "             'GCA':'A', 'GCG':'A', 'TAT':'Y', 'TAC':'Y', 'TAA':'*', 'TAG':'*', 'CAT':'H', 'CAC':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "             'AAT':'N', 'AAC':'N', 'AAA':'K', 'AAG':'K', 'GAT':'D', 'GAC':'D', 'GAA':'E', 'GAG':'E', 'TGT':'C', 'TGC':'C',\n",
    "             'TGA':'*', 'TGG':'W', 'CGT':'R', 'CGC':'R', 'CGA':'R', 'CGG':'R', 'AGT':'S', 'AGC':'S', 'AGA':'R', 'AGG':'R',\n",
    "             'GGT':'G', 'GGC':'G', 'GGA':'G', 'GGG':'G'}\n",
    "\n",
    "'''\n",
    "This function checks for WT allele (if Aligned_Sequence = Reference_Sequence)\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of merge dataframe \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "returns True if the allele is unedited (i.e. WT) and False otherwise\n",
    "\n",
    "'''\n",
    "def get_wt_col(row):\n",
    "    if row['Aligned_Sequence'] == row['Reference_Sequence']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''\n",
    "This function calculates the LFC\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : row of metainformation input file containing BEV_test and BEV_ref columns\n",
    "data_file : merged dataframe containing log-normalized rpm for each allele\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "data_file : merged dataframe, now with LFC columns for each BEV test / ref pair\n",
    "\n",
    "'''\n",
    "def get_lfc_v2(row,data_file):\n",
    "    cols = []\n",
    "    bev_list = row['BEV_test'].split(';')\n",
    "    \n",
    "    # Go through each test sample in BEV_test column\n",
    "    for i,bev in enumerate(bev_list):\n",
    "        test = get_bev_str(bev)\n",
    "        \n",
    "        # Get reference sample for LFC from BEV_ref column\n",
    "        ref = get_bev_str(row['BEV_ref'].split(';')[i])\n",
    "        \n",
    "        # Calculate LFC\n",
    "        data_file['LFC_'+test+'-'+ref] = data_file['#Reads_BEV_'+test+';lognorm'] - data_file['#Reads_BEV_'+ref+';lognorm']  \n",
    "        cols.append('LFC_'+test+'-'+ref)\n",
    "        \n",
    "    # Average together LFC columns\n",
    "    data_file['AvgLFC_'+'_'.join(bev_list)] = data_file.loc[:,cols].mean(axis=1)\n",
    "    return data_file\n",
    "\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: merges read counts (filtered), lognorms, aligned sequences, reference sequences, translations from \n",
    "test and reference samples for sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "data : deduplicated input file with column 'sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "merge : merged dataframe with filtered read counts, lognorms, aligned sequences, reference sequences, translations\n",
    "\n",
    "'''\n",
    "        \n",
    "def process_data_v2(data):\n",
    "    bev_list = [] # list to store info for retrieving CRISPResso files for given sgRNA\n",
    "    ref_nums = [] # list to store BEV numbers for CRISPResso files for reference samples \n",
    "    for i,row in data.iterrows():\n",
    "        for bev in range(row['BEV_start'],row['BEV_end']+1):\n",
    "            # store BEV number, sgRNA sequence, and primer name to get corresponding CRISPResso files \n",
    "            bev_list.append((get_bev_str(bev),row['sgRNA_sequence'],row['primer']))\n",
    "        \n",
    "        # if reference samples outside of BEV_start and BEV_end range, add to bev_list separately\n",
    "        BEV_ref = data['BEV_ref'].copy().tolist() #.copy.loc['BEV_ref']))\n",
    "        BEV_ref_split = BEV_ref[0].split(';')\n",
    "        for num in BEV_ref_split:\n",
    "            # check if reference sample numbers outside of BEV_start and BEV_end range\n",
    "            if ((int(num) < row['BEV_start']) | (int(num) > row['BEV_end'])):\n",
    "                ref_nums.append(num)\n",
    "            else:\n",
    "                continue\n",
    "        for ref_num in ref_nums:\n",
    "            # store BEV number, sgRNA sequence, and primer name to get corresponding CRISPResso files \n",
    "            bev_list.append((get_bev_str(ref_num),row['sgRNA_sequence'],row['primer']))\n",
    "        print(bev_list)\n",
    "    \n",
    "    # Call get_bev_files function which merges Alleles_frequency_tables_around_sgRNA for given sgRNA  \n",
    "    merge,cols = get_bev_files(bev_list)\n",
    "    \n",
    "    # Calculate log-normalized reads per million for each col\n",
    "    for col in ['#Reads'+col for col in cols]:\n",
    "        colsum = merge[col].sum()\n",
    "        merge.loc[:,str(col+';lognorm')] = merge[col].apply(lambda x: log((float(x)/float(colsum))*1000000 + 1,2))\n",
    "        \n",
    "    # Apply read count filter\n",
    "    merge.loc[:,'%read_count_filter'] = merge.apply(read_count_filter,args=(['%Reads'+col for col in cols],1),axis=1) # less than 1% of all reads\n",
    "    merge.loc[:,'#read_count_filter'] = merge.apply(read_count_filter,args=(['#Reads'+col for col in cols],100),axis=1) # less than 100 reads\n",
    "    \n",
    "    # Before translating, check if there are introns in ref_seq column in input file\n",
    "    # introns indicated by lowercase letters in ref_seq column \n",
    "    # Returns true if intron in ref_seq (i.e. if lowercase letters in input ref_seq)\n",
    "    intron_flag = any(c.islower() for c in str(data.loc[data.index[0], 'ref_seq']))\n",
    "    # if introns exist, remove introns before translating \n",
    "    if intron_flag:\n",
    "        print('introns exist')\n",
    "        intron_input_df = pd.DataFrame()\n",
    "        intron_input_df['Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].copy()\n",
    "        intron_input_df['ref_seq'] = data['ref_seq'].to_list()[0]\n",
    "        intron_input_df['rev_com'] = data.loc[i, 'rev_com']\n",
    "        \n",
    "        # Call remove_introns function to remove introns from aligned sequence before translating \n",
    "        intron_input_df['Translate_input']= list(map(remove_introns, intron_input_df['ref_seq'], intron_input_df['Aligned_Sequence'], intron_input_df['rev_com']))\n",
    "        \n",
    "        # Call translate function to translate new (intron-free) sequence  \n",
    "        merge.loc[:,'Translated'] = intron_input_df['Translate_input'].apply(translate, args =(row['frame'],codon_map,))\n",
    "\n",
    "    # if no introns\n",
    "    else:\n",
    "        # check if Aligned Sequence needs to be reverse complemented before translating \n",
    "        rev_com = data.loc[i, 'rev_com']\n",
    "        if rev_com:\n",
    "            merge.loc[:,'Aligned_Sequence'] = merge.loc[:,'Aligned_Sequence'].apply(be.revcom)\n",
    "            merge.loc[:,'Reference_Sequence'] = merge.loc[:,'Reference_Sequence'].apply(be.revcom)\n",
    "        # Call translate function to translate Aligned Sequence (reverse complemented if necessary) \n",
    "        merge.loc[:,'Translated'] = merge.loc[:,'Aligned_Sequence'].apply(translate,args=(row['frame'],codon_map,))\n",
    "\n",
    "    return merge\n",
    "'''\n",
    "Called from: run\n",
    "Function: joins all possible combinations of #Reads;lognorms columns from samples for given sgRNA \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "row : #Reads;lognorm columns \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "columns joined by '_' : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column'\n",
    "Ex. '#Reads_BEV_041;lognorm_#Reads_BEV_042;lognorm'  \n",
    "\n",
    "'''\n",
    "\n",
    "def get_corr_name(row):\n",
    "    cols = [row['R1'],row['R2']]\n",
    "    cols.sort()\n",
    "    return '_'.join(cols)\n",
    "\n",
    "'''\n",
    "Called from: run\n",
    "Function: gets combinations of samples specified in reps_for_correlation column in correlation input \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "corr_input : correlation input file \n",
    "sg: guide identifier from corr_input\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "combos : '#Reads_BEV_#;lognorm column_#Reads_BEV_#;lognorm column' for pairs specified in corr_input  \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def get_correlation_cols(corr_input,sg):\n",
    "    corr_input = corr_input.loc[corr_input['sg'] == sg,:]\n",
    "    combos = []\n",
    "    for i,r in corr_input.iterrows():\n",
    "        bevs = r['reps_for_correlation'].split(';')\n",
    "        bevs = ['#Reads_BEV_'+get_bev_str(bev)+';lognorm' for bev in bevs]\n",
    "        if len(bevs) > 1:\n",
    "            combos.extend(itertools.combinations(bevs,2))\n",
    "    combos = ['_'.join(combo) for combo in combos]\n",
    "    return combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function calls all previous functions to generate output tables \n",
    "\n",
    "INPUTS\n",
    "------\n",
    "input_file : input file with columns 'sg', 'ref_seq', 'sgRNA_sequence', 'BEV_start', 'BEV_end', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "corr_input : input file with columns 'sg', 'reps_for_correlation' described above\n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "output files stored in output_filepath given above\n",
    "\n",
    "'''\n",
    "\n",
    "def run(input_file,corr_input):\n",
    "    \n",
    "    # List of sgRNA identifiers provided in \"sg\" column of input file\n",
    "    sg_list = list(set(input_file['sg'].tolist())) # drop duplicates from list\n",
    "\n",
    "    # Go through each sgRNA separately    \n",
    "    for sg in sg_list:\n",
    "        print(sg)\n",
    "        \n",
    "        # Filter input file to contain only rows for given sgRNA\n",
    "        data = input_file.loc[input_file['sg'] == sg,:]\n",
    "        \n",
    "        # Merge all the allele read counts\n",
    "        # To do this, drop the BEV_test and BEV_ref columns (which just have information needed for the LFC calculation)\n",
    "        # Then drop duplicate rows\n",
    "        data_dedup = data.drop_duplicates(subset=['sg','BEV_start','BEV_end','sgRNA_sequence','primer','frame','rev_com'])\n",
    "        \n",
    "        # Call process_data_v2 function which adds the following columns to output table:\n",
    "        #read counts, lognorms, aligned sequences, reference sequences, translations \n",
    "        merge = process_data_v2(data_dedup)\n",
    "        \n",
    "        # Get the WT column\n",
    "        merge['WT'] = merge.apply(get_wt_col,axis=1)\n",
    "        \n",
    "        # Now, go through each row and calculate the LFC for each of the pairs specified in BEV_test and BEV_ref\n",
    "        for i,r in data.iterrows():               \n",
    "            merge = get_lfc_v2(r,merge)\n",
    "        #print(merge.head())\n",
    "            \n",
    "        # Write out 2 files: full file (merge) and filtered file (only including alleles with > 1% reads in at least one condition)\n",
    "        Path(output_filepath).mkdir(parents=True, exist_ok=True)\n",
    "        merge.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_allele_frequency_table_around_sgRNA.csv',index=False)\n",
    "        filtered = merge[merge['%read_count_filter'] == True]\n",
    "        filtered.to_csv(output_filepath +str(r['sg'])+'_'+r['primer']+'_filtered_allele_frequency_table_around_sgRNA.csv',\n",
    "                        index=False)            \n",
    "\n",
    "        # Get correlations matrix of log-normalized rpm, using only alleles with > 100 reads in at least one sample\n",
    "        merge = merge[merge['#read_count_filter'] == True]\n",
    "        cols = [x for x in list(merge) if 'lognorm' in x]\n",
    "        correlations = merge[cols].corr(method='pearson')\n",
    "        correlations['R1'] = correlations.index\n",
    "        correlations = correlations.melt(id_vars = 'R1', value_vars=list(correlations).remove('R1'),var_name='R2',value_name='Pearson')\n",
    "        \n",
    "        # Drop correlations that are not specified in corr_input\n",
    "        correlations['Reps'] = correlations.apply(get_corr_name,axis=1)\n",
    "        combos = get_correlation_cols(corr_input,sg)\n",
    "        correlations = correlations[correlations['Reps'].isin(combos)]\n",
    "        \n",
    "        # Drop duplicate rows (i.e. A vs B and B vs A)\n",
    "        correlations = correlations.drop_duplicates(subset=['Reps'])\n",
    "        \n",
    "        # Write to file\n",
    "        Path(output_filepath + \"corr_outputs/#read_count_filter_pearson/\").mkdir(parents=True, exist_ok=True)\n",
    "        correlations.to_csv(output_filepath + \"corr_outputs/#read_count_filter_pearson/sg\" +str(r['sg'])+'_'+r['primer']+'_correlations.csv')\n",
    "        \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the two input files and produce allele tables for all sgRNAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(input_file,corr_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'monospace' #so translated sequences are aligned\n",
    "\n",
    "'''\n",
    "This function generates allele-level heatmaps using log-fold changes for each validation condtion\n",
    "annotated with the corresponding allele amino acid sequences\n",
    "\n",
    "INPUTS\n",
    "------\n",
    "df : input file with columns 'sg', 'ref_seq', 'sgRNA_sequence', 'BEV_start', 'BEV_end', 'primer',\n",
    "       'frame', 'rev_com', 'BEV_ref', 'BEV_test' described above\n",
    "vmin : sets minimum value for heatmap color bar\n",
    "vmax: sets maximum value for heatmap color bar\n",
    "filepath: same as output_filepath specified above where allele frequency output tables are saved \n",
    "\n",
    "OUTPUTS\n",
    "-------\n",
    "allele-level heatmpas with corresponding allele amino acid sequences annotated \n",
    "\n",
    "'''\n",
    "\n",
    "def heatmaps(df,vmin,vmax,filepath, **kwargs):\n",
    "    if len(df) > 2:\n",
    "        #determine number of rows, cols (2 heatmaps to a row)\n",
    "        num_rows = math.ceil(len(df)/2)\n",
    "        num_cols = 2 \n",
    "        fig,axs = plt.subplots(num_rows,num_cols,figsize=(num_cols*4,num_rows*4))\n",
    "    else:\n",
    "        num_rows = 1\n",
    "        num_cols = len(df)\n",
    "        fig,ax = plt.subplots(num_rows,num_cols,figsize=(num_cols*4,num_rows*4))\n",
    "    \n",
    "    fig.subplots_adjust(wspace=5)\n",
    "    \n",
    "    row_count = 0\n",
    "    col_count = 0\n",
    "    last_row = False\n",
    "    for num, (i,r) in enumerate(df.iterrows()):\n",
    "        # check if incomplete row -> remainder 1 = True, remainder 0 = False\n",
    "        incomplete_row = bool(len(df)%2) \n",
    "        # flag if last row: \n",
    "        if incomplete_row:\n",
    "            if num == len(df) - 1:\n",
    "                last_row = True\n",
    "                #print('last row')\n",
    "        sg = r['sg']\n",
    "        #print('before plot sg: ', sg)\n",
    "        primer = r['primer']\n",
    "        location = filepath+str(sg)+'_'+primer+'_filtered_allele_frequency_table_around_sgRNA.csv'\n",
    "        to_plot = pd.read_csv(location)\n",
    "        \n",
    "        avgLFC_col = [col for col in to_plot.columns if 'AvgLFC' in col][0]\n",
    "\n",
    "        to_plot.sort_values(by=avgLFC_col,ascending=False,inplace=True)\n",
    "\n",
    "        to_plot['Avg%Reads_Time1'] = round(to_plot[to_plot.columns[to_plot.columns.str.contains('%Reads')][2:]].apply(np.mean,axis=1),1).apply(str)+'%'\n",
    "        \n",
    "        to_plot['Avg%Reads_Time2'] = round(to_plot[to_plot.columns[to_plot.columns.str.contains('%Reads')][:2]].apply(np.mean,axis=1),1).apply(str)+'%'\n",
    "        \n",
    "        to_plot['Label'] = to_plot['Translated'] #+ ' ' + to_plot['Avg%Reads_Time1'] + ' | ' + to_plot['Avg%Reads_Time2']   \n",
    "        \n",
    "        #reads_df = to_plot[['Avg%Reads_Time1', 'Avg%Reads_Time2']]\n",
    "        #print(reads_df)\n",
    "        \n",
    "        to_plot.loc[to_plot['WT'],'Label'] = 'wt>' + to_plot.loc[to_plot['WT'],'Label'].values[0] \n",
    "#         print(to_plot['Label'])    \n",
    "        wt_label = to_plot.loc[to_plot['WT'],'Label'].values[0]\n",
    "        heatmap_df = to_plot[['Label',[col for col in to_plot.columns if 'AvgLFC' in col][0]]]\n",
    "        #print(heatmap_df)\n",
    "        \n",
    "        if row_count < num_rows:\n",
    "            if col_count < num_cols: \n",
    "                if num_rows == 1:\n",
    "                    if num_cols == 1:\n",
    "                        sns.heatmap(heatmap_df.set_index('Label'),cmap='RdBu',vmin=vmin,vmax=vmax,ax=ax,annot=True,fmt='.1f')\n",
    "                        label_list = [item for item in ax.get_yticklabels()]\n",
    "                        for label in label_list:\n",
    "                            if 'wt>' in label.get_text():\n",
    "                                #print(label)\n",
    "                                label.set_color('red')\n",
    "                        plt.setp(ax.get_yticklabels(), rotation=0)#, ha=\"right\",rotation_mode=\"anchor\")\n",
    "                        ax.set_ylabel('', rotation=0)\n",
    "                        ax.set_title(sg)\n",
    "                    else:\n",
    "                        sns.heatmap(heatmap_df.set_index('Label'),cmap='RdBu',vmin=vmin,vmax=vmax,ax=ax[col_count],annot=True,fmt='.1f')\n",
    "                        label_list = [item for item in ax[col_count].get_yticklabels()]\n",
    "                        for label in label_list:\n",
    "                            if 'wt>' in label.get_text():\n",
    "                                #print(label)\n",
    "                                label.set_color('red')\n",
    "                        plt.setp(ax[col_count].get_yticklabels(), rotation=0)#, ha=\"right\",rotation_mode=\"anchor\")\n",
    "                        ax[col_count].set_ylabel('', rotation=0)\n",
    "                        ax[col_count].set_title(sg)\n",
    "                    \n",
    "                else: \n",
    "                    sns.heatmap(heatmap_df.set_index('Label'),cmap='RdBu',vmin=vmin,vmax=vmax,ax=axs[row_count, col_count],annot=True,fmt='.1f')\n",
    "                    label_list = [item for item in axs[row_count, col_count].get_yticklabels()]\n",
    "                    for label in label_list:\n",
    "                        if 'wt>' in label.get_text():\n",
    "                            #print(label)\n",
    "                            label.set_color('red')\n",
    "                    plt.setp(axs[row_count, col_count].get_yticklabels(), rotation=0, ha=\"right\")#,rotation_mode=\"anchor\")\n",
    "                    axs[row_count, col_count].set_ylabel('', rotation = 0)\n",
    "                    axs[row_count, col_count].set_title(sg)\n",
    "                if (col_count+1 == num_cols):\n",
    "                    col_count = 0 \n",
    "                    row_count += 1\n",
    "                else:\n",
    "                    col_count += 1\n",
    "                    if last_row: #remove blank subplot if incomplete row\n",
    "                        axs[row_count, col_count].remove()\n",
    "        #fig.align_ylabels()\n",
    "    # Create path to Figures folder if doesn't exist already\n",
    "    Path(filepath + '/Figures/').mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(filepath + '/Figures/translation_heatmap.pdf',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps(df = input_file,\n",
    "         vmin = -2,\n",
    "         vmax = 2,\n",
    "         filepath = output_filepath,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
